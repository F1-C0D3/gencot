
Parsing and processing C code in Gencot is always implemented in Haskell, to be able to use an existing
C parser. There are at least two choices for a C parser in Haskell:
\begin{itemize}
\item the package ``language-c'' by Benedikt Huber and others,
\item the package ``language-c-quote'' by **** Mainland.
\end{itemize}

The Cogent compiler uses the package language-c-quote for outputting the generated C code and for parsing the antiquoted
C source files. The reason is its support for quasiquotation (embedding C code in Haskell code) and antiquotation
(embedding Haskell code in the embedded C code). The antiquotation support is used for parsing the antiquoted C sources.

Gencot performs three tasks related to C code:
\begin{itemize}
\item read the original C code to be translated,
\item generate antiquoted C code for the function wrapper implementations,
\item output normal C code for the C function bodies as placeholder in the generated Cogent function definitions.
\end{itemize}

The first task is supported by both packages: a C parser reads the source text and creates an internal abstract syntax tree (AST).
Every package uses its own data structures for representing the AST. However, the language-c package provides an additional
``analysis'' module which processes the rather complicated syntax of C declarations and returns a ``symbol map'' mapping
every globally declared identifier to its declaration or definition. Since Gencot generates a single Cogent definition for
every single globally declared identifier, this is the ideal starting point for Gencot. For this reason Gencot uses
the language-c parser for the first task.

The second task is only supported by the package language-c-quote, therefore it is used by Gencot. 

The third task is supported by both packages, since both have a prettyprint function for outputting their AST. Since the 
function bodies have been read from the input and are output with only minor modifications, it is easiest to use
the language-c prettyprinter, since language-c has been used for parsing and the body is already represented by its 
AST data structures. However, the language-c prettyprinter is not generic enough to adapt it to Gencot's requirements.
For this reason Gencot uses an own reimplementation of the language-c prettyprinter for the third task (see 
Section~\ref{impl-ccode-expr}).

Note that in both packages the main module is named \code{Language.C}. If both packages are exposed to the ghc Haskell
compiler, a package-qualified import must be used in the Haskell program, which must be enabled by a language pragma:
\begin{verbatim}
  {-# LANGUAGE PackageImports #-}
  ...
  import "language-c" Language.C
\end{verbatim}

\subsection{Including Files}
\label{impl-ccode-include}

The filter \code{gencot-include <dirlist>} processes all quoted include directives and replaces them (transitively) by the 
content of the included file. Line directives are inserted at the begin and end of an included file, so that
for all code in the output the original source file name and line number can be determined. The \code{<dirlist>}
specifies the directories to search for included files.

\subsubsection{Filter \code{gencot-include}}

The filter for expanding the include directives is implemented as an awk script, heavily inspired by the ``igawk''
example program in the gawk infofile, edition 4.2, in Section 11.3.9.

As argument it expects a directory list specified with ``:'' as separator. The list corresponds
to directories specified with the \code{-I} cpp option, it is used for searching included files.
All directories for searching included files must be specified in the arguments, there are no defaults.

Similar to cpp, a file included by a quoted directive is first searched in the directory of the including file. 
If not found there, the argument directory list is searched.

Since the input of \code{gencot-include} is read from standard input it is not associated with a directory. Hence
if files are included from the same directory, that directory must also be specified explicitly in an argument directory
list.

\subsubsection{Generating Line Directives}

Line directives are inserted into the output as follows.

If the first line of the input is a line directive, it is copied to the output. Otherwise 
the line directive
\begin{verbatim}
  # 1 "<stdin>"
\end{verbatim}
is prepended to the output.

If after a generated line directive with file name \code{\"fff\"} the input line \code{NNN} contains the 
directive 
\begin{verbatim}
  #include "filepath"
\end{verbatim}
the directive is replaced in the output by the lines 
\begin{verbatim}
  # 1 "dir/filepath" 1
  <content of file filepath>
  # NNN+1 "fff" 2
\end{verbatim}

The \code{\"dir/\"} prefix in the line directives for included files is determined as follows. 
If the included file has been found in the 
directory of its includer, the directory pathname is constructed from \code{\"fff\"} by taking the pathname 
up to and including the last ``/'' (if present, otherwise the prefix is empty).
If the included file has been found in a directory from the argument directory list
the directory pathname is used as specified in the list.

\subsubsection{Multiple Includes}

The C preprocessor does not prevent a file from being included multiple times. Usually, C include files use
an ifdef directive around all content to prevent multiple includes. The \code{gencot-include} filter does
not interprete ifdef directives, instead, it simply prevents multiple includes for all files independent 
from their contents, only based on their full file pathnames. To mimic the behavior of cpp, if a file is 
not include due to repeated include, the corresponding line directives are nevertheless generated in the form
\begin{verbatim}
  # 1 "dir/filepath" 1
  # NNN+1 "fff" 2
\end{verbatim}

\subsection{Preprocessing}
\label{impl-ccode-preproc}

The language-c parser supports an integrated invocation of an external preprocessor, the default is to use
the gcc preprocessor. However, the integrated invocation always reads the C code from a file (and checks
its file name extension) and not from standard input.

To implement C code processing as a filter, Gencot does not use the integrated preprocessor,
it invokes the preprocessor as an additional separate step. For consistency reasons it is wrapped in
the minimal filter script \code{gencot-cpp}. 

The preprocessor step only has the following purpose:
\begin{itemize}
\item process all system include directives by including the file contents,
\item process retained conditional directives to prevent conflicts in the C code.
\end{itemize}
All other preprocessing has already been done by previous steps.

\subsection{Reading the Input}
\label{impl-ccode-read}

\subsubsection{Parsing}

To apply the language-c parser to the standard input we invoke it using function \code{parseC}. It needs an \code{InputStream}
and an initial \code{Position} as arguments. 

The language-c parser defines \code{InputStream} to be the standard type \code{Data.ByteString}. To get the 
standard input as a \code{ByteString} the function \code{ByteString.getContents} can be used. 

The language-c parser uses type \code{Position} to describe a character position in a named file. It provides
the function \code{initPos} to create an initial position at the beginning of a file, taking a \code{FilePath}
as argument, which is a \code{String} containing the file name. Since Gencot and the C preprocessor create
line directives with the file name \code{<stdin>} for the standard input, this string is the correct argument
for \code{initPos}. 

The result of \code{parseC} is of type \code{(Either ParseError CTranslUnit)}. Hence it should be checked whether
an error occurred during parsing. If not, the value of type \code{CTranslUnit} is the abstract syntax tree for
the parsed C code.

Both \code{parseC} and \code{initPos} are exported by module \code{Language.C}. The function \code{ByteString.getContents}
is exported by the module \code{Data.Bytestring}. Hence to use the parser we need the following imports:
\begin{verbatim}
  import Data.ByteString (getContents)
  import "language-c" Language.C (parseC,initPos)
\end{verbatim}

Then the abstract syntax tree can be bound to variable \code{ast} using
\begin{verbatim}
  do
    input_stream <- Data.ByteString.getContents
    ast <- either (error . show) return $ parseC input_stream (initPos "<stdin>")
\end{verbatim}

\subsubsection{Analysis}

Although it is not complete and only processes toplevel declarations (including typedefs), and object definitions, the
language-c analysis module is very
useful for implementing Gencot translation. Function definition bodies are not covered by analysis, but they are
not covered by Gencot either.

The result of the analysis module is a map for all toplevel declarations and object definition, mapping the identifier
to its semantics, which is mainly its declared type. Whereas in the abstract syntax tree there may be several declarators
in a declaration, declaring identifiers with different types derived from a common type, the map maps every identifier
to its fully derived type. 

Also, tags for structs, unions and enums are contained in the map. In C their definitions can be embedded in other declarations.
The analysis module collects all these possibly embedded declarations in the map. The map also gives for every defined type name
the definition.

Together, the information in the map is much more appropriate for creating Cogent code, where all type definitions are on
toplevel. Therefore, Gencot uses the map resulting from the analysis step as starting point for its translation.

To use the analysis module, the following import is needed:
\begin{verbatim}
  import Language.C.Analysis
\end{verbatim}

Then, if the abstract syntax tree has been bound to variable \code{ast}, it can be analysed by
\begin{verbatim}
  globals <- either (error . show) (return . fst) $ runTrav_ $ analyseAST ast
\end{verbatim}
which binds the resulting map to variable \code{globals}. \code{runTrav\_}
returns a result of type \code{Either [CError] (GlobalDecls, [CError])}, where \code{GlobalDecls}
is the type of the semantics map. The error list in the first alternative contains fatal errors which made the analysis fail. 
The error list in the second alternative contains warnings about semantic inconsistencies, such as unknown identifiers,
which are returned together with the map. 

\subsubsection{Source Code Origin}

The language-c parser adds information about the source code origin to the AST. For every syntactic construct represented
in the AST it includes the start origin of the first input token and the start origin and length of the last input token.
The start origin of a token is represented by the type \code{Position} and includes the original source file name and 
line number, affected by line directives if present in the input. It also includes the absolute character offset in the 
input stream. The latter can be used to determine the ordering of constructs which have been placed in the same line.
The type \code{Position} is declared as instance of class \code{ORD} by comparing the character offset, hence it can 
easily be used for comparing and sorting.

The origin information about the first and last token is contained in the type \code{NodeInfo}. All types for representing
a syntactic construct in the AST are parameterized with a type parameter. In the actual AST types this parameter is always 
substituted by the type \code{NodeInfo}. 

The analysis module carries the origin information over to its results, by including a \code{NodeInfo} in most of its
result structures. This information can be used to
\begin{itemize}
\item determine the origin file for a declared identifier,
\item filter declarations according to the source file containing them,
\item sort declarations according to the position of their first token in the source,
\item translate identifiers to file specific names to avoid conflicts.
\end{itemize}

For the last case the true name of the processed file is required, however, the parsed input is read from a pipe where
the name is always given as \code{<stdin>}. The true name is passed to the Haskell program as an additional 
argument, as described in Section~\ref{impl-ccode-filters}. Since there is no easy way to replace the file name in
all \code{NodeInfo} values in the semantic map, Gencot adds a pseudo declaration for the identifier \code{<stdin>}
(which is no valid C identifier) to the semantic map. It is mapped to a dummy declaration together with a \code{NodeInfo}
which contains the true file name.

\subsubsection{Preparing for Processing}

The main task for Gencot is to translate all declarations or definitions which are contained in a single source file, where
nested declarations are translated to a sequence of toplevel Cogent definitions. This is achieved by parsing and analysing
the content of the file and all included files, filtering the resulting set of declarations according to the source file name
\code{<stdin>}, removing all declarations which are not translated to Cogent, and sorting the remaining in a list. 
Translating every list entry to Cogent yields the resulting Cogent definitions in the correct ordering.

The type \code{GlobalDecls} consists of three separate maps, one for tag definitions, one for type definitions,
and one for all other declarations and definitions. Every map uses its own type for its range values, however, 
there is the wrapper type \code{DeclEvent} which has a variant for each of them. 

The language-c analysis module provides a filtering function for its resulting map of type \code{GlobalDecls}. The filter 
predicate is defined for values of type \code{DeclEvent}. If the map has been bound to the variable \code{globals}, 
as described above, it can be filtered by
\begin{verbatim}
  filterGlobalDecls globalsFilter gmap
\end{verbatim}
where \code{globalsFilter} is the filter predicate.

Gencot uses a filter which reduces the declarations to those directly in the input file, removing all
content from included files. Since the input file is always associated with the name \code{<stdin>} in the \code{NodeInfo}
values, a corresponding filter function is
\begin{verbatim}
  (maybe False ((==) "<stdin>") . fileOfNode)
\end{verbatim}
Additionally, the declarations are reduced to those which are processed by the specific Gencot processor.

Every map range value, and hence every \code{DeclEvent} value contains the identifier which is mapped to it, 
hence the full information required for translating the definitions is contained in the range values. 
Gencot wraps every range value as a \code{DeclEvent}, and puts them in a common list for all three maps. This
is done by the function
\begin{verbatim}
  listGlobals :: LCA.GlobalDecls -> [LCA.DeclEvent]
  listGlobals gmap = 
      (concat $ map elems $ wraps <*> [gmap])
      ++ (elems $ fmap LCA.TagEvent $ gTags gmap)
      where wraps = [(fmap LCA.DeclEvent) . gObjs, 
                     (fmap LCA.TypeDefEvent) . gTypeDefs]
\end{verbatim}

Finally, the declarations in the list are sorted according to the offset position of their first tokens, using the
compare function
\begin{verbatim}
  compEvent :: DeclEvent -> DeclEvent -> Ordering
  compEvent ci1 ci2 = compare (posOf ci1) (posOf ci2)
\end{verbatim}

Together, the list for processing the code is prepared from map \code{globals} by
\begin{verbatim}
  sortBy compEvent $ listGlobals $ filterGlobalDecls globalsFilter gmap
\end{verbatim}

All this preprocessing is implemented by module \code{Gencot.Input}. It provides the three functions
\begin{verbatim}
  readFromInput :: IO GlobalDecls
  readFromFile :: FilePath -> IO GlobalDecls
  getDeclEvents :: GlobalDecls -> (DeclEvent -> Bool) -> [DeclEvent]
\end{verbatim}
The \code{readFrom*} functions parse and analyse the standard input or a file content, respectively. The function
\code{getDeclEvents} performs the remaining preprocessing and returns the list of \code{DeclEvent}s to be processed.
As its second argument it expects a predicate for filtering the content of \code{<stdin>} to the
\code{DeclEvent}s to be processed by the specific Gencot processor.

\subsection{Generating Cogent Code}
\label{impl-ccode-gencog}

When Gencot generates its Cogent target code it uses the data structures defined by the Cogent compiler for representing
its AST after parsing Cogent code. The motivation to do so is twofold. First, the AST omits details such as using code layout
and parentheses for correct code structure and the Cogent compiler provides a prettyprint function for its AST which cares
about these details. Hence, it is much easier to generate the AST and use the prettyprinter for output, instead of generating
the final Cogent program text. Second, by using the Cogent AST the generated Cogent code is guaranteed to be syntactically correct and
current for the Cogent language version of the used compiler version. Whenever the Cogent language syntax is changed
in a newer version, this will be detected when Gencot is linked to the newer compiler version.

\subsubsection{Cogent Surface Syntax Tree}

The data structures for the Cogent surface syntax AST are defined in the module Cogent.Surface. It defines parameterized types
for the main Cogent syntax constructs (\code{TopLevel}, \code{Alt}, \code{Type}, \code{Polytype}, \code{Pattern}, 
\code{IrrefutablePattern}, \code{Expr}, and \code{Binding}, where the type parameters determine the types of the 
sub-structures. Hence the AST types
can easily be extended by wrapping the existing types in own extensions which are then also used as actual type parameters.

Cogent itself defines two such wrapper type families: The basic unextended types \code{RawXXX} and the types \code{LocXXX}
where every construct is extended by a representation of its source location. 

All parameterized types for syntax constructs are defined as instances of \code{Traversable} for every type parameter.
All these types and the \code{RawXXX} and \code{LocXXX} types are defined as instances of class \code{Pretty} from
module \code{Text.PrettyPrint.ANSI.Leijen}. This prettyprinter functionality is used by the Cogent compiler for outputting
the parsed Cogent source code after some processing steps, if requested by the user.

As source location representation in the \code{LocXXX} types Cogent uses the type \code{SourcePos} from Module 
\code{Text.Parsec.Pos} in package \code{parsec}.
It contains a file name and a row and column number. This information is ignored by the prettyprinter.

\subsubsection{Extending the Cogent Surface Syntax}

Gencot needs to extend the Cogent surface syntax for its generated code in two ways:
\begin{itemize}
\item origin markers must be supported, as described in Section~\ref{impl-origin},
\item C function bodies must be supported in Cogent function definitions, as described in Section~\ref{design-fundefs-body}.
\end{itemize}

The origin markers are used to optionally surround the generated target code parts, which may be arbitrary syntactic constructs
or groups of them. Hence it would be necessary to massively extend the Cogent surface syntax, if they are added as explicit 
syntactic constructs. Instead, Gencot optionally adds the information about the range of source lines to the syntactic
constructs in the AST and generates the actual origin markers when the AST is output. 

Although the \code{LocXXX} types already support a source position in every syntactic construct, it cannot be used by Gencot,
since it represents only a single position instead of a line range. Gencot uses the \code{NodeInfo} values, since they represent
a line range and they are already present in the C source code AST, as described in Section~\ref{impl-ccode-read}. Hence, they
can simply be transferred from the source code part to the corresponding target code part. For the case that there is no
source code part in the input file (such as for code generated for external name references), the \code{NodeInfo} is optional.

It may be the case that a source code part follows another part in the same line. Then, although the \code{NodeInfo} specifies 
that line as beginning line, no \code{\#ORIGIN} marker must be generated, since it has already been generated for a previous 
source code part (the first one in the line), or must not be generated at all, since the line beginning is an inner position
in an unstructured part. The analogous property holds for the \code{\#ENDORIG} marker, however, the presence of both markers
is independent from each other.

It may also be the case that a structured source code part is translated to a sequence of sub-part translations without target
code for the main part. In this case the \code{\#ORIGIN} marker for the main part must be added before the \code{\#ORIGIN} 
marker of the first target code part and the \code{\#ENDORIG} marker for the main part must be added after the \code{\#ENDORIG} 
marker of the last target code part. 

To represent all these cases, the origin information for a construct in the target AST consists of two lists of \code{NodeInfo}
values. The first list represents the sequence of \code{\#ORIGIN} markers to be inserted before the construct, here only the
start line numbers in the \code{NodeInfo} values are used. The second list represents the sequence of \code{\#ENDORIG} markers 
to be inserted after the construct, here only the end line numbers in the \code{NodeInfo} values are used. If no marker of
one of the kinds shall be present, the corresponding list is empty.

Additional Information must be added to represent the marker extensions for placing the comments (the trailing ``+'' signs).
Therefore, a boolean value is added to all list elements.

Together, Gencot defines the type \code{Origin} for representing the origin information, with the value \code{noOrigin}
for the case that no markers will be generated:
\begin{verbatim}
  data Origin = Origin { 
    sOfOrig :: [(NodeInfo,Bool)], 
    eOfOrig :: [(NodeInfo,Bool)] } 
  noOrigin = Origin [] []
\end{verbatim}
Gencot adds an \code{Origin} value to every Cogent AST element.

Cogent function definitions are represented by the \code{FunDef} alternative of the type for toplevel syntactic constructs:

\begin{verbatim}
  data TopLevel t p e = 
    ... | FunDef VarName (Polytype t) [Alt p e] | ...
\end{verbatim}
The type parameter \code{e} for representing syntactic expressions is only used in this alternative and in the alternative
for constant definitions. Cogent constant definitions are generated by Gencot only from C enum constants (preprocessor
constants are processed by \code{gencot-prcconst} which is not implemented in Haskell). The defined value for a C enum
constant is represented in the C AST by the type for expressions. Together, instead of Cogent expressions, Gencot always
uses either a C expression or a C function body (which syntactically is a statement) in the Cogent AST. 

To modify the Cogent syntax in this way, Gencot defines an own expression type with two alternatives for a C expression 
and a C statement:
\begin{verbatim}
  data GenExpr = ConstExpr Expr
               | FunBody Stmt
\end{verbatim}
where \code{Expr} and \code{Stmt} are the types for C expressions and statements as defined by the language-c analysis
module. Note that no \code{Origin} components are added, since the types \code{Expr} and \code{Stmt} already contain
\code{NodeInfo} components and in both cases only a single target code part is generated so that it is always the
target for comments. The Cogent AST expression type is not used by Gencot. Since bindings only occur in expressions,
the AST type for Cogent bindings is not used either.

For the type parameters \code{t} and \code{p} for representing types and patterns, respectively, the normal types for 
the Cogent constructs are used, since Gencot generates both in Cogent syntax. The pattern generated for a function
definition is always a tuple pattern, which is irrefutable. Gencot never generates other patterns, hence the AST
type for irrefutable patterns is sufficient. 

Together, Gencot uses the following types to represent its extended Cogent surface AST:
\begin{verbatim}
  data GenToplv =
    GenToplv Origin (TopLevel GenType GenIrrefPatn GenExpr)
  data GenAlt =
    GenAlt Origin (Alt GenIrrefPatn GenExpr)
  data GenIrrefPatn = 
    GenIrrefPatn Origin (IrrefutablePattern VarName GenIrrefPatn)
  data GenType = 
    GenType Origin (Type GenExpr GenType)
  data GenPolytype = 
    GenPolytype Origin (Polytype GenType)
\end{verbatim}
The first parameter of \code{Type} for expressions is only used for Cogent array types, which are currently 
not generated by Gencot.

All five wrapper types are defined as instances of class \code{Pretty}, basically by applying the Cogent prettyprint
functionality to the wrapped Cogent AST type.

\subsection{Mapping Names}
\label{impl-ccode-names}

Names used in the target code are either mapped from a C identifier or introduced, as described in 
Section~\ref{design-names}. Different schemas are used depending on the kind of name to be generated.
The schemas require different information as input.

\subsubsection{General Name Mapping}

The general mapping scheme is applied whenever a Cogent name is generated from an existing C identifier.
Its purpose is to adjust the case, if necessary and to avoid conflicts between the Cogent name and
the C identifier.

As input this scheme only needs the C identifier and the required case for the Cogent name.
It is implemented by the function
\begin{verbatim}
  mapName :: Bool -> Ident -> String
\end{verbatim}
where the first argument specifies whether the name must be uppercase.

\subsubsection{Cogent Type Names}

A Cogent type name (including the names of primitive types) may be generated as translation of a C 
primitive type, a C typedef name, a C struct/union/enum type reference, or a C derived type. 

A C primitive type is translated according to the description in Section~\ref{design-types}. Only the
type specifiers for the C type are required for that.

A C typedef name is translated by simply mapping it with the help of \code{mapName} to an uppercase name.
Only the C typedef name is required for that.

A C struct/union/enum type reference may be tagged or tagless. If it is tagged, the Cogent type name is
constructed from the tag as described in Section~\ref{design-names}: the tag is mapped with the help of
\code{mapName} to an uppercase name, then a prefix \code{Struct\_}, \code{Union\_} or \code{Enum\_} is 
prepended. For this mapping the tag and the kind (struct/union/enum) are required. Both are contained
in the language-c type \code{TypeName} which is used to represent a reference to a struct/union/enum.

If the reference is untagged, Gencot nevertheless generates a type name, as motivated and described 
in Section~\ref{design-names}. As input it needs the kind and the position of the struct/union/enum 
definition. The latter is not contained in the \code{TypeName}, it contains the position of the reference
itself. To access the position of the definition, the definition must be retrieved from the 
\code{GlobalDecls} map created by the language-c analysis. Hence, this map must be provided as additional 
argument.

Together the function for translating struct/union/enum type references is
\begin{verbatim}
  transTagName :: GlobalDecls -> TypeName -> String
\end{verbatim}

Since an untagged struct/union/enum can be contained in any type specification and type specifications
may occur in all other C constructs, the \code{GlobalDecls} map must be passed as argument to all translation
functions from C constructs to Cogent constructs.

If the definition itself is translated, it is already available and need not be retrieved from the map. 
However, as described in Section~\ref{impl-ccode-gencog}, the map may be needed to map the generic name
\code{<stdin>} to the true source file name. Therefore Gencot uses function \code{transTagName} also when
translating the definition.

A C derived type is translated to a Cogent type name by translating the name of the basic type as described
above, and then prepending the encoded sequence of derivation steps, as
described in Section~\ref{design-names}. The information about the derivation steps is contained in the 
type construct, no information in addition to that required for translating the basic type name is needed.

\subsubsection{Cogent Function Names}

Cogent function names are generated from C function names. A C function may have external or internal
linkage, according to the linkage the Cogent name is constructed either as a global name or as a name specific
to the file where the function is defined. For deciding which variant to use for a function name reference,
its linkage must be determined. It is available in the definition or in a declaration for the function name,
either of which must be present in the \code{GlobalDecls} map. The language-c analysis module replaces all 
declarations in the map by the
definition, if that is present in the parsed input, otherwise it retains a declaration. 

A global function name is generated by mapping the C function name with the help of \code{mapName} to
a lowercase Cogent name. No additional information is required for that.

For generating a file specific function name, the file name of the definition is required. Note that 
this is only done for a function with internal linkage, where the definitions must be present in
the input whenever the function is referenced. The definition contains the position information
which includes the file name. Hence, the \code{GlobalDecls} map is sufficient for translating the name,
it is passed as additional argument. The function for translating a function name is
\begin{verbatim}
  transFunName :: GlobalDecls -> Ident -> String
\end{verbatim}

Similar as for tags, the function is also used when translating a function definition, although the 
definition is already available.

\subsubsection{Cogent Constant Names}

Cogent constant names are only generated from C enum constant names. They are simply translated
with the help of \code{mapName} to a lowercase Cogent name. No additional information is required.

\subsubsection{Cogent Field Names}

C member names and parameter names are translated to Cogent field names. Only if the C name is
uppercase, the name is mapped to a lowercase Cogent name with the help of \code{mapName}, 
otherwise it is used without change. Only the C name is required for that, in both cases it is
available as a value of type \code{Ident}. The translation is implemented by the function
\begin{verbatim}
  transToField :: Ident -> String
\end{verbatim}

\subsection{Generating Origin Markers}
\label{impl-ccode-origin}

For outputting origin markers in the target code, the AST prettyprint functionality must be extended.

The class \code{Pretty} used by the Cogent prettyprinter defines the methods
\begin{verbatim}
  pretty :: a -> Doc
  prettyList :: [a] -> Doc
\end{verbatim}
but the method \code{prettyList} is not used by Cogent. Hence, only the method \code{pretty} needs to be defined
for instances. The type \code{Doc} is that from module \code{Text.PrettyPrint.ANSI.Leijen}.

The basic approach is to wrap every syntactic construct in a sequence of \code{\#ORIGIN} markers and 
a sequence of \code{\#ENDORIG} markers according to the origin information for the construct in the extended AST. 
This is done by an instance definition of the form
\begin{verbatim}
  instance Pretty GenToplv where
    pretty (GenToplv org t) = addOrig org $ pretty t
\end{verbatim}
for \code{GenToplv} and analogous for the other types. The function \code{addOrig} has the type
\begin{verbatim}
  addOrig :: Origin -> Doc -> Doc
\end{verbatim}
and wraps its second argument in the origin markers according to its first argument.

The origin markers must be positioned in a separate line, hence \code{addOrig} outputs a newline before and after
each marker. To avoid unnecessary newlines, \code{addOrig} tests whether the current position before a marker 
is already at the beginning of a line, then the leading newline is omitted. The test is performed using the
function \code{column} from module \code{Text.PrettyPrint.ANSI.Leijen} which provides access to the current column 
position.

There are two issues with this approach: indentation and repeated origin markers.

\subsubsection{Indented Target Code Parts}

The Cogent prettyprinter uses indentation for subexpressions. Indentation is implemented by the \code{Doc} type, 
where it is called ``nesting''. The prettyprinter maintains a
current nesting level and inserts that amount of spaces whenever a new line starts. Hence, if a syntactic construct
is nested the nesting also applies to the origin markers, whereas the markers are always expected at the beginning of
a line. This can be dealt with using negative nesting for the markers.

Since a nesting change only becomes effective after the next newline, the negative nesting for a marker must be set 
before the 
leading newline for a marker is output. This implies that the leading newline can only be omitted if the current
nesting level is 0. This leads to additional newlines, in particular between consecutive origin markers. However,
this situation cannot be safely detected, since Cogent may change the nesting of the next line after \code{addOrig}
has output a marker (typically after an \code{\#ENDORIG} marker). The newline at the end of the previous marker 
still inserts spaces according to the old nesting level, which determines the current position at the begin of
the following marker. This is not related to the new nesting level, hence to unnest the following marker an additional
newline is required.

Gencot solves this by an additional postprocessing step which removes blank lines after \code{\#ENDORIG} markers.

\subsubsection{Repeated Origin Markers}

Normally, target code is positioned in the same order as the corresponding source code. This implies, that
origin markers are monotonic. A repeated origin marker is a marker with the same line number as its previous marker.
Repeated origin markers of the same kind must be avoided, since they would result in duplicated comments or 
misplaced directives.
Repeated origin markers of the same kind occur, if a subpart of a structured source code part begins or ends 
in the same line as its main part. In this case only the outermost markers are retained.

An \code{\#ENDORIG} marker repeating an \code{\#ORIGIN} marker means that the source code
part occupies only one single line (or a part of it), this is a valid case. 
An \code{\#ORIGIN} marker repeating an \code{\#ENDORIG} marker means that the previous source code
part ends in the same line where the following source code part begins. In this case the markers are
irrelevant, since no comments or directives can be associated with them. However, if they are
present they introduce unwanted line breaks, hence they also are avoided by removing both of them.

Together, the following rules result. In a sequence of repeated \code{\#ORIGIN} markers, only the first one 
is generated. In a sequence of repeated \code{\#ENDORIG} markers only the last one is generated.
If an \code{\#ORIGIN} marker repeats an \code{\#ENDORIG} marker, both are omitted.

There are several possible approaches for omitting repeated origin markers:
\begin{itemize}
\item omit repeated markers when building the Cogent AST
\item traverse the Cogent AST and remove markers to be omitted
\item output repeated markers and remove them in a postprocessing step
\end{itemize}
Note, that it is not possible to remove repeated markers already in the language-c AST, since there a \code{NodeInfo}
value always corresponds to two combined markers.

Gencot uses the first approach, since it seems to support the most specific handling of markers. However, the other
approaches seem to be possible as well.

Omitting repeated markers is implemented by passing the line numbers which would cause repeated markers to the 
functions building the Cogent AST. For the \code{\#ORIGIN} marker it is the last line of the previous sibling
(corresponding to a repeated \code{\#ENDORIG} marker), or the first line of the parent if there is no previous
sibling (corresponding to a repeated \code{\#ORIGIN} marker). Analogously, for the \code{\#ENDORIG} marker
it is the first line of the next sibling or the last line of the parent. If either line number is not available,
0 is used, since that is no valid line number. The pair of line numbers is represented by the type
\begin{verbatim}
  type RepOrig = (Int,Int)
\end{verbatim}

The following functions are used to create such pairs:
\begin{verbatim}
  mkRepOrig :: CNode a => a -> RepOrig
  fstRepOrig :: CNode a => a -> a -> RepOrig
  midRepOrig :: CNode a => a -> a -> RepOrig
  lstRepOrig :: CNode a => a -> a -> RepOrig
\end{verbatim}
The \code{CNode} class is defined by language-c for all types 
having an associated \code{NodeInfo} value, including type \code{NodeInfo} itself. It allows to retrieve the 
associated \code{NodeInfo} for every instance. 
Function \code{mkRepOrig} retrieves the first and last line number from the associated \code{NodeInfo}. 
The other functions create the required pairs for subconstructs from parent and siblings, where for 
\code{fstRepOrig} and \code{lstRepOrig} the first argument is the parent.


The case of an \code{\#ORIGIN} marker repeating an \code{\#ENDORIG} marker is handled as follows. For every case where 
a sequence of C constructs is translated, first the corresponding sequence of \code{NodeInfo}s is retrieved, then it is 
converted to the corresponding sequence of \code{Origin}s, and finally every origin value is passed to the translation 
function for the corresponding C construct where it is inserted into the target construct. 

The conversion of \code{NodeInfo} sequences to \code{Origin} sequences is implemented by folding a list of \code{CNode}
instances, where all repeated markers are omitted. The \code{CNode} class is defined by language-c for all types 
having an associated \code{NodeInfo} value, including type \code{NodeInfo} itself. It allows to retrieve the 
associated \code{NodeInfo} for every instance. Hence the list can either be
a list of syntactic constructs of the same type, or a list of \code{NodeInfo} values retrieved from syntactic 
constructs of different types. The conversion is implemented by the function
\begin{verbatim}
  listOrigins :: CNode a => [a] -> [Origin]
\end{verbatim}

Repeated markers of the same kind are handled as follows. When a structured source code part is translated, for every
subpart the \code{Origin} to be passed to its translation function is constructed by combining the subpart's 
\code{NodeInfo} with the main part's \code{NodeInfo}, omitting markers which repeat those of the main part. 
This is implemented by the function
\begin{verbatim}
  subOrigin :: CNode a => NodeInfo -> a -> Origin
\end{verbatim}
Note that the main part's original \code{NodeInfo} must be used for testing for repeated markers, the \code{Origin} 
to be inserted in the target code may already omit markers because they are also repeated.

If a structured source code part has a list of subparts, both functions must be combined by first generating origins 
for the list members as by \code{listOrigins}, and then remove markers repeated from the main part as by \code{subOrigin}.
Both steps together are implemented by the function
\begin{verbatim}
  subListOrigins :: CNode a => NodeInfo -> [a] -> [Origin]
\end{verbatim}

This approach implies that for translating a C construct first the \code{NodeInfo} values for all subconstructs are 
converted to \code{Origin} values, omitting all repeated markers. Then the subconstructs are translated recursively, using
the computed \code{Origin} values. Finally, the target code for the main construct is generated, integrating the translations
of the subconstructs. Hence every translation function for a C construct with an associated \code{NodeInfo} value
has an additional argument for the precomputed \code{Origin} value.

For the case that a construct has subconstructs of different types or optional subconstructs the following utility 
functions are defined:
\begin{verbatim}
  sub1l1Origins :: (CNode a1, CNode a, CNode a2) => 
    NodeInfo -> a1 -> [a] -> a2 -> (Origin,[Origin],Origin)
  sub2Origins :: (CNode a1, CNode a2) => 
    NodeInfo -> a1 -> a2 -> (Origin,Origin)
  sub3Origins :: (CNode a1, CNode a2, CNode a3) => 
    NodeInfo -> a1 -> a2 -> a3 -> (Origin,Origin,Origin)
  subListMaybeOrigins :: CNode a => 
    NodeInfo -> [Maybe a] -> [Maybe Origin]
\end{verbatim}

In language-c there are constructs generated by the analysis phase which have no associated \code{NodeInfo} value. An example
are type specifications. Such constructs cannot be treated in the way described above, when they occur as subconstructs.
Since no origin information is available for them, no origin markers can be generated and no \code{Origin} value is
passed to their translation function. The \code{Origin} value in the target code is always set to \code{noOrigin}.

However, a construct without associated \code{NodeInfo} may have subconstructs which have an associated \code{NodeInfo}.
An example are the parameter declarations in a function type. For these subconstructs origin markers should be generated.

If for a structured source code part no target code is generated for the main part before or after the subpart targets, the markers
for the main parts must be added to the first and/or last subpart target. This is done by the functions
\begin{verbatim}
  prependOrigin :: Origin -> Origin -> Origin
  apppendOrigin :: Origin -> Origin -> Origin
\end{verbatim}
where the first argument is the main part's \code{Origin}. These functions are applied to \code{Origin} values where all repeated 
markers have already been removed.

All these functions are defined together with type \code{Origin} in the module \code{Gencot.Origin}.

\subsection{Generating Expressions}
\label{impl-ccode-expr}

For outputting the Cogent AST the prettyprint functionality must be extended to 
output C function bodies and the C expressions used for constant definitions. Additionally, at least in function bodies,
origin markers must be generated to be able to re-insert comments and preprocessor directives.

The language-c prettyprinter is defined in module \code{Language.C.Pretty}. It defines an own class \code{Pretty} with 
method \code{pretty} to convert the AST types to a \code{Doc}. However, other than the Cogent prettyprinter, it uses 
the type \code{Doc} from module \code{Text.PrettyPrint.HughesPJ} instead of module \code{Text.PrettyPrint.ANSI.Leijen}.
This could be adapted by rendering the \code{Doc} as a string and then prettyprinting this string to a \code{Doc}
from the latter module. This way, a prettyprinted function body could be inserted in the document created by the
Cogent prettyprinter.

For generating origin markers, a similar approach is not possible, since they must be inserted between single statements,
hence, the function \code{pretty} must be extended. Although it does not use the \code{NodeInfo}, it is only defined for
the AST type instances with a \code{NodeInfo} parameter and has no genericity which could be exploited for extending it.
Therefore, Gencot has to fully reimplement it. 

There are several possible approaches how to structure this reimplementation. A straightforward way would be to generate
origin markers directly from the \code{NodeInfo} information. However, as described in Section~\ref{impl-ccode-origin},
repeated origin markers must be suppressed in the generated output. This cannot be decided locally, additional context
information is required during the AST traversal. In particular, the suppression of an \code{\#ENDORIG} marker depends
on whether it is \textit{followed} by a marker for the same line, which can be arbitrary far after the current 
AST element.

For the generated Cogent code this problem has already been solved with the help of the functions \code{listOrigins, 
subOrigins, subListOrigins, prependOrigin, appendOrigin}, as described in Section~\ref{impl-ccode-origin}. The goal
for the generated C code is to reuse these implementations. This can be done by traversing the AST and replacing 
all \code{NodeInfo} values by \code{Origin} values with suppressed repeated markers. This is possible, since the
language-c AST types are generic with \code{NodeInfo} as actual type parameter. However, language-c implements 
no traversal function for the AST type, this must be fully implemented in Gencot.

Now the prettyprint reimplementation can generate the markers from the \code{Origin} values in the AST, which is done
in the same way as for the Cogent AST: the AST element is wrapped by the markers specified in its \code{Origin} value.
Therefore, the function \code{addOrig} can be used, as described in Section~\ref{impl-ccode-origin}. This requires 
that the generated \code{Doc} is that from \code{Text.PrettyPrint.ANSI.Leijen}. Therefore, the reimplementation
generates this \code{Doc} instead of that in the original implementation. This adaptation is easy to implement,
since all operator for \code{Text.PrettyPrint.HughesPJ.Doc} are also available for 
\code{Text.PrettyPrint.ANSI.Leijen.Doc} with often even the same syntax.

Together this would require two traversals of the language-c AST: one for replacing \code{NodeInfo} values by 
\code{Origin} values and suppressing repeated markers, and another one for the prettyprint function. To simplify
the reimplementation, Gencot combines both traversals in one. Instead of actually replacing \code{NodeInfo} values
by \code{Origin} values in the AST elements, the \code{Origin} value for each AST element is created on the fly 
and passed as an additional parameter to the prettyprint invocation for the AST element.

Due to the additional \code{Origin} parameter, the reimplementation cannot use the class \code{Pretty} used 
by the Cogent prettyprinter. Gencot defines its own class \code{OPretty}
\begin{verbatim}
  class OPretty p where
    pretty :: p -> Origin -> Doc
    prettyPrec :: Int -> p -> Origin -> Doc
\end{verbatim}
where the methods are implemented for all AST types like in the language-c prettyprinter with additionally
generating the origin markers.

\subsection{Filters for C Code Processing}
\label{impl-ccode-filters}

All filters which parse and process C code are implemented in Haskell and read the
C code as described in Section~\ref{impl-ccode-read}.

The following filters always process the content of a single C source file and produce the content for a single 
target file.
\begin{description}
\item[\code{gencot-translate}] translates a single file \code{x.c} or \code{x.h} to the Cogent code to be put in file
\code{x-impl.cogent} or \code{x-types.cogent}. It processes typedefs, struct/union/enum definitions, and function
definitions. 
\item[\code{gencot-globals}] translates a single file \code{x.c} to the Cogent code to be put in file \code{x-globals.cogent}.
It processes all global variable definitions.
\item[\code{gencot-entries}] translates a single file \code{x.c} to the antiquoted C entry wrappers to be put in
file \code{x-entry.ac}. It processes all function definitions with external linkage.
\item[\code{gencot-abstypes}] translates a single file \code{x.c} or \code{x.h} to the C typedefs to be put in
file \code{x-abstypes.c} or \code{x-abstypes.h}. It processes typedefs and struct/union/enum definitions.
\item[\code{gencot-remfundef}] processes a single file \code{x.c} by removing all function definitions. The output
is intended to be put in file \code{x-globals.c}
\end{description}

The filters \code{gencot-translate} and \code{gencot-abstypes} take the name of the original source file as additional
argument, since they need it to generate Cogent names for C names with internal linkage and for tagless C struct/union
types.

There are other target files which are generated for the whole package. The filters for generating these target files 
must always determine and process the external name references in a set of
C source files. This set is the subset of C sources in the <package> which is translated to Cogent and together yields
the Cogent compilation unit. There are different possible approaches how to read and process this set of source files.

The first approach is to use a single file which includes all files in the set. This file is processed as usual by
\code{gencot-include}, \code{gencot-remcomments}, and \code{gencot-rempp} which yields the union of all definitions
and declarations in all files in the set as input to the language-c parser. However, this input may contain conflicting
definitions. For an identifier with internal linkage different definitions may be present in different source files.
Also for identifiers with no linkage different definitions may be present, if, e.g., different \code{.c} files define
a type with the same name. The language-c parser ignores duplicate definitions for identifiers with internal linkage,
however, it treats duplicate definitions for identifiers without linkage as a fatal error. Hence Gencot does not use
this approach.

The second approach ist to process every file in the set separately and merge the generated target code. However, for
identifiers with external linkage (function definitions) the external references cannot be determined from the content
of a single file. A non-local reference is only external if it is not defined in any of the files in the set. It would
be possible to determine these external references in a separate processing step and using the result as additional input
for the main processing step. Since this means to additionally implement reading and writing a list of external references,
Gencot does not use this approach.

The third approach is to parse and analyse the content of every file separately, then merge the resulting semantic maps
discarding any duplicate definitions. This approach assumes that the external name references, which are relevant for
processing, are uniquely defined in all source files. If this is not the case, because conflicting definitions are used
inside the <package>, which are external to the processed file subset, this must be handled manually. Note that the
external references must be determined before the maps are merged, since they may occur in conflicting definitions
which are discarded during the merge. This approach is used by Gencot.

Due to the approach used, the Gencot ``filters'' for generating the files common to the Cogent compilation unit are
actually no filters, they take a list of file names as arguments and are called ``processors'' in the following.
Like all other input to the language-c parser their
content must have been processed by \code{gencot-include}, \code{gencot-remcomments}, and \code{gencot-rempp}.

Usually it is sufficient to specify only \code{.c} files in the set, since the information about all referenced 
identifiers must be provided in included \code{.h} files. However, for determining which references are external, the
\code{.h} files are needed as well, to distinguish between definitions provided by them and definitions provided
by other \code{.h} files not belonging to the set. The \code{.h} files need not be parsed, since their content is
already parsed together with the content of the \code{.c} files, only their names must be known. Hence the Gencot
processors distinguish the argument file names according to their file name extension: if the extension is 
\code{.gencot} a preprocessed \code{.c} file is expected to be parsed and processed, if the extension is \code{.h}
an original include file is expected and only its name is used for determining external name references.

Gencot uses the following processors of this kind:
\begin{description}
\item[\code{gencot-exttypes}] generates the content to be put in the file \code{<package>-exttypes.cogent}. It 
processes externally referenced typedefs, tag definitions and enum constant definitions.
\item[\code{gencot-absext}] generates the content to be put in the file \code{<package>-exttypes.c}. It 
processes externally referenced typedefs, tag definitions and enum constant definitions.
\item[\code{gencot-exit}] generates the exit wrappers to be put in the file \code{<package>-exit.ac}. It processes
the declarations of externally referenced functions.
\item[\code{gencot-exitabs}] generates the abstract function definitions to be put in the file 
\code{<package>-exit.cogent}. It processes the declarations of externally referenced functions.
\item[\code{gencot-extincludes}] generates the list of include directives to be put in the file
\code{<package>-extincludes.c}. It processes all external name references. 
\end{description}

Additionally, Gencot uses the following filter for postprocessing generated target code with embedded origin markers:
\begin{description}
\item[\code{gencot-postproc}] postprocesses the origin markers generated by the other filters and processors.
\end{description}

\subsection{Main Translation to Cogent}
\label{impl-ccode-main}

The main translation from C to Cogent is implemented by the filter \code{gencot-translate}. It translates \code{DeclEvent}s
of the following kinds:
\begin{itemize}
\item struct/union definitions
\item enum definitions with a tag
\item enum constant definitions
\item function definitions
\item type definitions
\end{itemize}
The remaining global items are removed by the predicate passed to \code{Gencot.Input.getDeclEvents}: all declarations, 
all object definitions, and all tagless enum definitions. No Cogent type name is generated for a tagless enum definition,
references to it are always directly replaced by type \code{U32}.

The translation of the \code{DeclEvent} sequence is implemented by the function
\begin{verbatim}
  transGlobals :: GlobalDecls -> [DeclEvent] -> [GenToplv]
\end{verbatim}
where the first argument is used for name mapping as described in Section~\ref{impl-ccode-names}.

A struct/union/enum definition corresponds to a full specifier, as described in Section~\ref{design-decls-tags}.
The language-c analyser already implements moving all full specifiers to separate global definitions and the
sorting step done by \code{Gencot.Input.getDeclEvents} creates the desired ordering. Therefore, only the translation
of the single struct/union/enum definitions has to be implemented by \code{gencot-translate}.

A struct/union definition is translated to a Cogent type definition where the type name is constructed as described 
in Section~\ref{design-names}. A struct is translated to a corresponding record type, a union is translated to an 
abstract type, as described in Section~\ref{design-types}.
In both cases the type name names the boxed type, i.e., it corresponds to the C type of a pointer to the struct/union.

An enum definition with a tag is translated to a Cogent type definition where the type name is constructed as described 
in Section~\ref{design-names}. The name is always defined for type \code{U32}, as described in Section~\ref{design-types}.

An enum constant definition is translated to a Cogent constant definition where the name is constructed as described 
in Section~\ref{design-names} and the type is always \code{U32}, as described in Section~\ref{design-types}.

A function definition is translated to a Cogent function definition, as described in Section~\ref{design-fundefs}.

A type definition is translated to a Cogent type definition as described in Section~\ref{design-decls-typedefs}.

All these single translations are implemented by the function
\begin{verbatim}
  transGlobal :: GlobalDecls -> DeclEvent -> Origin -> GenToplv
\end{verbatim}
where the first argument is used for name mapping as described in Section~\ref{impl-ccode-names} and the third
argument is the \code{Origin} to be used, with repeated markers removed as described in Section~\ref{impl-ccode-origin}.

A type reference is translated by the function
\begin{verbatim}
  transType :: GlobalDecls -> Type -> NodeInfo -> GenType 
\end{verbatim}
where the first argument is used for name mapping and the third argument is the original origin information of
the C construct which has the type reference as a subpart. It is used for suppressing repeated origin markers
as described in Section~\ref{impl-ccode-origin}.

A type reference may be a direct type, a derived type, or a typedef name. For every typedef name a Cogent type
name is defined, as described in Sections~\ref{design-names} and~\ref{design-modular}. A direct type is either
the type \code{void}, a primitive C type, which is mapped to the name of a primitive Cogent type, or it is a 
struct/union/enum type reference for which Gencot also introduces a Cogent type name or maps it to the 
primitive Cogent type \code{U32} (tagless enums). Hence, both direct types and typedef names can always be mapped
to Cogent type names, with the exception of type \code{void}, which is mapped to the Cogent unit type \code{()}.

If a typedef name references (directly or indirectly) a struct or union type, the corresponding Cogent
type name references the boxed type. Therefore, it must be modified by applying the unbox operator. If a typedef
name references a primitive type, this is not necessary, since the corresponding Cogent type is always regular.
However, the abstract Cogent surface syntax always associates a ``sigil'' with a type name. Unnecessary
unbox operators are automatically suppressed by the Cogent prettyprint function. Therefore Gencot always 
associates an unbox sigil with the Cogent type name if it corresponds to a direct type or a typedef name
referencing a direct type.

A derived type is either a pointer type, an array type, or a function type. It is derived from a base type
which in case of a function type is the type of the function result. The base type may again be a derived
type, ultimately it is a direct type or a typedef name.

For a pointer type the translation depends on the base type. If it is a struct or union type or a typedef
name referencing a struct or union type, the pointer type is translated to the Cogent type name corresponding
to the base type. If it is a function type or a typedef name referencing a function type, the pointer type 
is translated to the translation of the base type. In all other cases, as described in Section~\ref{design-types}, 
the pointer type is translated to the name of an abstract type, which is introduced as described in 
Section~\ref{design-names}, using a name for the base type. 

For an array type, the translation also depends on the base type. If it is type \code{char}, the array 
type is translated to the primitive Cogent type \code{String}. In all other cases it is translated 
to the name of an abstract type, which is introduced as described in Section~\ref{design-names}, 
using a name for the base type. This is even done if the base type is a typedef name which references
type char, since Gencot assumes that in this case it is intended as a ``real'' array type and not
as a string type.

If an array type occurs as type of a function parameter, it is ``adjusted'' by C to a pointer type with
the same base type. This is also done by Gencot. 

A function type is always translated to the corresponding Cogent function type, where a tuple type is
used as parameter type if there is more than one parameter, and the unit type is used if there is
no parameter. 

If a pointer type or array type is translated to the name of an abstract type, a name is required for
the base type. If the base type is a direct type or a typedef name, a Cogent type name always exists 
and is used. The only exception is type \code{void}, here the name \code{Void} is used.
If the base type is a derived type, a name is constructed for it as described in Section~\ref{design-names},
even if the base type would normally be mapped to a type expression (which is the case for a function type)
or to its own base type (which is the case for pointer types to struct/union and function types). If the
base type is array of char, the name \code{String} is used for it. 

Note that for most cases where a typedef name occurs as reference or base type, it must be resolved to
the ultimate direct type or derived type referenced by it. This is implemented by the function
\begin{verbatim}
  resolveTypeDef :: TypeDefRef -> Type 
\end{verbatim}
The \code{GlobalDecls} map is not required here, since the language-c analyser puts the (directly) 
referenced type in the \code{TypeDefRef}. 

For a qualified C type Gencot only respects the \code{const} qualifier. For a direct type the \code{const}
qualifier is ignored, since in Cogent values of unboxed and regular types are always immutable. For
a function type the qualifier is also ignored since function types are regular in Cogent. For an array
of char it is ignored since it is translated to type \code{String} which is regular.

All other array types and all pointer types are translated to linear types which can be mutable in
Cogent. Whenever the C type contains no mutable pointer types, it is translated to a readonly Cogent type by 
applying a bang operator to it.

An array type contains mutable pointers if its base type does so. A function type never contains mutable pointers.
A pointer type contains mutable pointers if its base type is not \code{const} qualified or contains mutable pointers.
A primitive type and an enum type does not contain mutable pointers. A struct or union type contains mutable 
pointers, if the type of a member contains mutable pointers.

