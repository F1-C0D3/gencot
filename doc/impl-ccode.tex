
Parsing and processing C code in Gencot is always implemented in Haskell, to be able to use an existing
C parser. There are at least two choices for a C parser in Haskell:
\begin{itemize}
\item the package ``language-c'' by Benedikt Huber and others,
\item the package ``language-c-quote'' by Geoffrey Mainland and others.
\end{itemize}

The Cogent compiler uses the package language-c-quote for outputting the generated C code and for parsing the antiquoted
C source files. The reason is its support for quasiquotation (embedding C code in Haskell code) and antiquotation
(embedding Haskell code in the embedded C code). The antiquotation support is used for parsing the antiquoted C sources.

Gencot performs three tasks related to C code:
\begin{itemize}
\item read the original C code to be translated,
\item generate antiquoted C code for the function wrapper implementations,
\item output normal C code for the C function bodies as placeholder in the generated Cogent function definitions.
\end{itemize}

The first task is supported by both packages: a C parser reads the source text and creates an internal abstract syntax tree (AST).
Every package uses its own data structures for representing the AST. However, the language-c package provides an additional
``analysis'' module which processes the rather complicated syntax of C declarations and returns a ``symbol map'' mapping
every globally declared identifier to its declaration or definition. Since Gencot generates a single Cogent definition for
every single globally declared identifier, this is the ideal starting point for Gencot. For this reason Gencot uses
the language-c parser for the first task.

The second task is only supported by the package language-c-quote, therefore it is used by Gencot. 

The third task is supported by both packages, since both have a prettyprint function for outputting their AST. Since the 
function bodies have been read from the input and are output with only minor modifications, it is easiest to use
the language-c prettyprinter, since language-c has been used for parsing and the body is already represented by its 
AST data structures. However, the language-c prettyprinter cannot be extended to generate the ORIGIN markers, therefore
the AST is translated to the language-c-quote AST and the corresponding prettyprinter is used for the third task (see 
Section~\ref{impl-ccode-expr}).

Note that in both packages the main module is named \code{Language.C}. If both packages are exposed to the ghc Haskell
compiler, a package-qualified import must be used in the Haskell program, which must be enabled by a language pragma:
\begin{verbatim}
  {-# LANGUAGE PackageImports #-}
  ...
  import "language-c" Language.C
\end{verbatim}

\subsection{Including Files}
\label{impl-ccode-include}

The filter \code{gencot-include <dirlist> [<filename>]} processes all quoted include directives and replaces them (transitively) by the 
content of the included file. Line directives are inserted at the begin and end of an included file, so that
for all code in the output the original source file name and line number can be determined. The \code{<dirlist>}
specifies the directories to search for included files. The optional \code{<filename>}, if present, must be the name of
a file with a list of names of files which should be omitted from being included.

\subsubsection{Filter \code{gencot-include}}

The filter for expanding the include directives is implemented as an awk script, heavily inspired by the ``igawk''
example program in the gawk infofile, edition 4.2, in Section 11.3.9.

As argument it expects a directory list specified with ``:'' as separator. The list corresponds
to directories specified with the \code{-I} cpp option, it is used for searching included files.
All directories for searching included files must be specified in the arguments, there are no defaults.

Similar to cpp, a file included by a quoted directive is first searched in the directory of the including file. 
If not found there, the argument directory list is searched.

Since the input of \code{gencot-include} is read from standard input it is not associated with a directory. Hence
if files are included from the same directory, that directory must also be specified explicitly in an argument directory
list.

\subsubsection{Generating Line Directives}

Line directives are inserted into the output as follows.

If the first line of the input is a line directive, it is copied to the output. Otherwise 
the line directive
\begin{verbatim}
  # 1 "<stdin>"
\end{verbatim}
is prepended to the output.

If after a generated line directive with file name \code{"fff"} the input line \code{NNN} contains the 
directive 
\begin{verbatim}
  #include "filepath"
\end{verbatim}
the directive is replaced in the output by the lines 
\begin{verbatim}
  # 1 "dir/filepath" 1
  <content of file filepath>
  # NNN+1 "fff" 2
\end{verbatim}

The \code{"dir/"} prefix in the line directives for included files is determined as follows. 
If the included file has been found in the 
directory of its includer, the directory pathname is constructed from \code{"fff"} by taking the pathname 
up to and including the last ``/'' (if present, otherwise the prefix is empty).
If the included file has been found in a directory from the argument directory list
the directory pathname is used as specified in the list.

\subsubsection{Multiple Includes}

The C preprocessor does not prevent a file from being included multiple times. Usually, C include files use
an ifdef directive around all content to prevent multiple includes. The \code{gencot-include} filter does
not interprete ifdef directives, instead, it simply prevents multiple includes for all files independent 
from their contents, only based on their full file pathnames. To mimic the behavior of cpp, if a file is 
not include due to repeated include, the corresponding line directives are nevertheless generated in the form
\begin{verbatim}
  # 1 "dir/filepath" 1
  # NNN+1 "fff" 2
\end{verbatim}

\subsubsection{Omitted Includes}

A special case of multiple include is the recursive include of the main input file. However, since it is read from standard
input, its name is not known to \code{gencot-include}. If it may happen that it is recursively included, the corresponding
pathname, as it appears in an include directive, must be added to the list of includes to be omitted.

There are other reasons, why some include files should be omitted. One case is that an include file may or may not exists
which is configured through a preprocessor flag. Since \code{gencot-include} ignores all conditional directives,
this would not be detected and an error message would be caused if the file does not exist.

The list of files to be omitted from inclusion is specified in the optional file passed as second argument to 
\code{gencot-include}. Every file must be specified on a separate line by its pathname exactly in the form it occurs 
in a quoted include directive (without quotes). Since system includes and includes where the included file is specified 
as a macro call are not processed by \code{gencot-include} they need not be added to the list.

Includes for files listed to be omitted are simply ignored. No line
directives are generated for them.

\subsection{Preprocessing}
\label{impl-ccode-preproc}

The language-c parser supports an integrated invocation of an external preprocessor, the default is to use
the gcc preprocessor. However, the integrated invocation always reads the C code from a file (and checks
its file name extension) and not from standard input.

To implement C code processing as a filter, Gencot does not use the integrated preprocessor,
it invokes the preprocessor as an additional separate step. For consistency reasons it is wrapped in
the minimal filter script \code{gencot-cpp}. 

The preprocessor step only has the following purpose:
\begin{itemize}
\item process all system include directives by including the file contents,
\item process retained conditional directives to prevent conflicts in the C code.
\end{itemize}
All other preprocessing has already been done by previous steps.

\subsection{Reading the Input}
\label{impl-ccode-read}

\subsubsection{Parsing}

To apply the language-c parser to the standard input we invoke it using function \code{parseC}. It needs an \code{InputStream}
and an initial \code{Position} as arguments. 

The language-c parser defines \code{InputStream} to be the standard type \code{Data.ByteString}. To get the 
standard input as a \code{ByteString} the function \code{ByteString.getContents} can be used. 

The language-c parser uses type \code{Position} to describe a character position in a named file. It provides
the function \code{initPos} to create an initial position at the beginning of a file, taking a \code{FilePath}
as argument, which is a \code{String} containing the file name. Since Gencot and the C preprocessor create
line directives with the file name \code{<stdin>} for the standard input, this string is the correct argument
for \code{initPos}. 

The result of \code{parseC} is of type \code{(Either ParseError CTranslUnit)}. Hence it should be checked whether
an error occurred during parsing. If not, the value of type \code{CTranslUnit} is the abstract syntax tree for
the parsed C code.

Both \code{parseC} and \code{initPos} are exported by module \code{Language.C}. The function \code{ByteString.getContents}
is exported by the module \code{Data.Bytestring}. Hence to use the parser we need the following imports:
\begin{verbatim}
  import Data.ByteString (getContents)
  import "language-c" Language.C (parseC,initPos)
\end{verbatim}

Then the abstract syntax tree can be bound to variable \code{ast} using
\begin{verbatim}
  do
    input_stream <- Data.ByteString.getContents
    ast <- either (error . show) return $ parseC input_stream (initPos "<stdin>")
\end{verbatim}

\subsubsection{Analysis}

Although it is not complete and only processes toplevel declarations (including typedefs), and object definitions, the
language-c analysis module is very
useful for implementing Gencot translation. Function definition bodies are not covered by analysis, but they are
not covered by Gencot either.

The main result of the analysis module is the symbol table. Since at the end of traversing a correct C AST the toplevel
scope is reached, the symbol table only contains all globally defined identifiers. From this symbol table 
a map is created containing all toplevel declarations and object definitions, mapping the identifiers
to their semantics, which is mainly its declared type. Whereas in the abstract syntax tree there may be several declarators
in a declaration, declaring identifiers with different types derived from a common type, the map maps every identifier
to its fully derived type. 

Also, tags for structs, unions and enums are contained in the map. In C their definitions can be embedded in other declarations.
The analysis module collects all these possibly embedded declarations in the symbol table. The map also gives for
every defined type name its definition.

Together, the information in the map is much more appropriate for creating Cogent code, where all type definitions are on
toplevel. Therefore, Gencot uses the map resulting from the analysis step as starting point for its translation. 
Additionally, Gencot uses the symbol table built by the analysis module during its own processing to access the
types of globally defined identifiers and for managing local declarations when traversing function bodies, as described in
Section~\ref{impl-ccode-trav}.

Additionally, the analysis module provides a callback handler which is invoked for every declaration entered into the symbol 
table (with the exception of tag forward declarations and enumerator declarations). The callback handler can accumulate results 
in a user state which can be retrieved after analysis together with the
semantics map. Since the callback handler is also invoked for all local declarations it is useful when all declarations
shall be processed in some form.

To use the analysis module, the following import is needed:
\begin{verbatim}
  import Language.C.Analysis
\end{verbatim}

Then, if the abstract syntax tree has been bound to variable \code{ast}, it can be analysed by
\begin{verbatim}
  (table,state) <- either (error . show) return $ 
    runTrav uinit (withExtDeclHandler (analyseAST ast >> getDefTable) uhandler)
\end{verbatim}
which binds the resulting symbol table to variable \code{table} and the resulting state to \code{ustate}. \code{runTrav}
returns a result of type \code{Either [CError] (DefTable, TravState s)}, where \code{DefTable}
is the type of the symbol table and \code{s} is the type of the user state. The error list in the first alternative contains 
fatal errors which made the analysis fail. The state in the second alternative contains warnings about semantic inconsistencies, 
such as unknown identifiers, and it contains the user state. \code{uinit} is the initial user state and \code{uhandler}
is the callback handler of type
\begin{verbatim}
  DeclEvent -> Trav s ()
\end{verbatim}
It returns a monadic action without result.

The semantics map is created from the symbol table by the function \code{globalDefs}, its type is \code{GlobalDecls}.

On this basis, Gencot implements the following functions in the module \code{Gencot.Input} as utility for parsing and analysis:
\begin{verbatim}
  readFromInput :: s -> (DeclEvent -> Trav s ()) -> IO (DefTable, s)
  readFromFile :: FilePath -> s -> (DeclEvent -> Trav s ()) -> IO (DefTable, s)
\end{verbatim}
The first one takes as arguments an initial user state and a callback handler. It reads C code from standard input, parses
and analyses it and returns the symbol table and the user state accumulated by the callback handler. The second function
takes a file name as additional argument and does the same reading from the file.

All Gencot filters which read C code use one of these two functions.

\subsubsection{Source Code Origin}

The language-c parser adds information about the source code origin to the AST. For every syntactic construct represented
in the AST it includes the start origin of the first input token and the start origin and length of the last input token.
The start origin of a token is represented by the type \code{Position} and includes the original source file name and 
line number, affected by line directives if present in the input. It also includes the absolute character offset in the 
input stream. The latter can be used to determine the ordering of constructs which have been placed in the same line.
The type \code{Position} is declared as instance of class \code{ORD} by comparing the character offset, hence it can 
easily be used for comparing and sorting.

The origin information about the first and last token is contained in the type \code{NodeInfo}. All types for representing
a syntactic construct in the AST are parameterized with a type parameter. In the actual AST types this parameter is always 
substituted by the type \code{NodeInfo}. 

The analysis module carries the origin information over to its results, by including a \code{NodeInfo} in most of its
result structures. This information can be used to
\begin{itemize}
\item determine the origin file for a declared identifier,
\item filter declarations according to the source file containing them,
\item sort declarations according to the position of their first token in the source,
\item translate identifiers to file specific names to avoid conflicts.
\end{itemize}

For the last case the true name of the processed file is required, however, the parsed input is read from a pipe where
the name is always given as \code{<stdin>}. The true name is passed to the Haskell program as an additional 
argument, as described in Section~\ref{impl-ccomps-filters}. Since there is no easy way to replace the file name in
all \code{NodeInfo} values in the semantic map, Gencot adds the name to the monadic state used for processing
(see Section~\ref{impl-ccode-trav}).

\subsubsection{Preparing for Processing}

The main task for Gencot is to translate all declarations or definitions which are contained in a single source file, where
nested declarations are translated to a sequence of toplevel Cogent definitions. This is achieved by parsing and analysing
the content of the file and all included files, filtering the resulting set of declarations according to the source file name
\code{<stdin>}, removing all declarations which are not translated to Cogent, and sorting the remaining ones in a list. 
Translating every list entry to Cogent yields the resulting Cogent definitions in the correct ordering.

For syntactically nested constructs in C the analysis phase creates separate declarations. This corresponds to the 
Cogent form where every declaration becomes a seperate toplevel construct. However, for generating the origin information 
a seperate processing of these declarations would yield repeated origin ranges which may result in repeated comment units in
the target code. Therefore Gencot processes nested 
constructs as part of the containing constructs. Sorting the declarations according to their positions always puts the
nested declarations after their containing declarations. Processing them as part of the containing declaration will
always be done before the nested declaration occurs in the main list of declarations. By maintaining a list of declarations
already processed as nested, Gencot skips these declarations when it finds them in the main list.

The only C constructs which can be nested are tag definitions for struct, union, and enum types (all other cases of nesting
occurs only in C function bodies where the nested parts are not contained as seperate entries in the main list). Tag definitions
can occur as part of every type specification, the most important case is the occurrence in a typedef as in
\begin{verbatim}
  typedef struct s { ... } t
\end{verbatim}
Other relevant cases are the occurrence in the declaration of a global variable, in the result or parameter types of a 
function declaration and in the declaration of a struct or union member (which may result in arbitrarily deep nesting).

For these cases, if the type references a tag definition, Gencot inspects the position information of the tag definition.
If it is syntactically embedded, it processes it and marks it as processed so that it is skipped when it finds it in the
main list.

The type \code{GlobalDecls} consists of three separate maps, one for tag definitions, one for type definitions,
and one for all other declarations and definitions. Every map uses its own type for its range values, however, 
there is the wrapper type \code{DeclEvent} which has a variant for each of them. 

The language-c analysis module provides a filtering function for its resulting map of type \code{GlobalDecls}. The filter 
predicate is defined for values of type \code{DeclEvent}. If the map has been bound to the variable \code{gmap}
it can be filtered by
\begin{verbatim}
  filterGlobalDecls globalsFilter gmap
\end{verbatim}
where \code{globalsFilter} is the filter predicate.

Gencot uses a filter which reduces the declarations to those contained directly in the input file, removing all
content from included files. Since the input file is always associated with the name \code{<stdin>} in the \code{NodeInfo}
values, a corresponding filter function is
\begin{verbatim}
  (maybe False ((==) "<stdin>") . fileOfNode)
\end{verbatim}
Additionally, for a specific Gencot component, the declarations are reduced to those which are processed by the component. 

Every map range value, and hence every \code{DeclEvent} value contains the identifier which is mapped to it, 
hence the full information required for translating the definitions is contained in the range values. 
Gencot wraps every range value as a \code{DeclEvent}, and puts them in a common list for all three maps. This
is done by the function
\begin{verbatim}
  listGlobals :: GlobalDecls -> [DeclEvent]
\end{verbatim}

Finally, the declarations in the list are sorted according to the offset position of their first tokens, using the
compare function
\begin{verbatim}
  compEvent :: DeclEvent -> DeclEvent -> Ordering
  compEvent ci1 ci2 = compare (posOf ci1) (posOf ci2)
\end{verbatim}

Together, the list for processing the code is prepared from the symbol table \code{table} by
\begin{verbatim}
  sortBy compEvent $ listGlobals $ filterGlobalDecls globalsFilter $ globalDefs table
\end{verbatim}

All this preprocessing is implemented in module \code{Gencot.Input}. It provides the function
\begin{verbatim}
  getDeclEvents :: GlobalDecls -> (DeclEvent -> Bool) -> [DeclEvent]
\end{verbatim}
It performs the preprocessing and returns the list of \code{DeclEvent}s to be processed.
As its second argument it expects a predicate for filtering the content of \code{<stdin>} to the
\code{DeclEvent}s to be processed by the specific Gencot component.

\subsection{Reading Packages}
\label{impl-ccode-package}

In some cases several source files of the <package> must be processed together. The typical case is when the main
files for the Cogent compilation unit are generated (see Section~\ref{design-files}). For this it is necessary 
to determine and process the external name references in a set of
C source files. This set is the subset of C sources in the <package> which is translated to Cogent and together yields
the Cogent compilation unit. 

\subsubsection{General Approach}

There are different possible approaches how to read and process this set of source files.

The first approach is to use a single file which includes all files in the set. This file is processed as usual by
\code{gencot-include}, \code{gencot-remcomments}, and \code{gencot-rempp} which yields the union of all definitions
and declarations in all files in the set as input to the language-c parser. However, this input may contain conflicting
definitions. For an identifier with internal linkage different definitions may be present in different source files.
Also for identifiers with no linkage different definitions may be present, if, e.g., different \code{.c} files define
a type with the same name. The language-c parser ignores duplicate definitions for identifiers with internal linkage,
however, it treats duplicate definitions for identifiers without linkage as a fatal error. Hence Gencot does not use
this approach.

The second approach ist to process every file in the set separately and merge the generated target code. However, for
identifiers with external linkage (function definitions) the external references cannot be determined from the content
of a single file. A non-local reference is only external if it is not defined in any of the files in the set. It would
be possible to determine these external references in a separate processing step and using the result as additional input
for the main processing step. Since this means to additionally implement reading and writing a list of external references,
Gencot does not use this approach.

The third approach is to parse and analyse the content of every file separately, then merge the resulting semantic maps
discarding any duplicate definitions. This approach assumes that the external name references, which are relevant for
processing, are uniquely defined in all source files. If this is not the case, because conflicting definitions are used
inside the <package>, which are external to the processed file subset, this must be handled manually. 
This approach is used by Gencot.

\subsubsection{Specifying Input Files}

Due to the approach used, the Gencot filters for generating the files common to the Cogent compilation unit expect
as input a list of names of the files which comprise the Cogent compilation unit. The file names must be pathnames which are either 
absolute or relative to the current directory. Every file name must occur on a single line.

Like all other input to the language-c parser their content must have been processed by \code{gencot-include}, 
\code{gencot-remcomments}, and \code{gencot-rempp}. This implies that all included content is already present
and need not be specified separately, usually only \code{.c} files need to be specified as input, after they have
been processed as usual.

In contrast to the single-file filters, the original file name is not required for the files input to a 
Gencot processor. A processor only processes items which are external to all input files, whereas the original
file name is needed for items which are defined in the input files. Therefore the list of input file names is
sufficient and the input files may have arbitrary names which need not be related to the names of the original 
\code{.c} files.

The utility function
\begin{verbatim}
  readPackageFromInput :: IO [DefTable]
\end{verbatim}
in module \code{Gencot.Package} reads the file name list from input and parses and analyses all files using 
\code{readFromFile}. It returns the list of resulting symbol tables.

\subsubsection{Combining Parser Results}

When the parser results are combined it is relevant, how they are structured, in particular, if the same file
is included by several of the \code{.c} files. Most of the information only depends on the parsed text.
However, the language-c parser also uses unique identifiers, which are counted integers starting at 1 for every
parsed file. This implies, that these identifiers are not unique anymore, if several files are parsed separately
and then the results are combined.

The unique identifiers are associated with most of the AST nodes and are part of the \code{NodeInfo} values.
In the analysis phase they are used to cache the relation between defining and referencing occurrences of C identifiers,
and the relation between C expressions and their types. The corresponding caches are part of the symbol table structure.
However, the first relation cache seems to be built but not used, the second relation cache is only used during 
type analysis for expressions. In both cases, after the analysis phase the caches are still present, but are not 
relevant for the further processing by Gencot.

As a consequence, it is not possible to combine the raw AST structures and then perform the language-c analysis
on the combined AST, since then the non-unique identifiers may cause problems. Instead, Gencot parses and analyses
every file separately and the combines the resulting symbol tables.

However, the unique identifiers are additionally used for identifying tagless struct/union/enum types in the symbol table.
Language-c uses the alternative type \code{SUERef} to identify struct/union/enum types, with the alternatives 
\code{NamedRef} and \code{AnonymousRef}, where the latter specifies the unique integer identifier of the AST node
of the type definition. The symbol table maps \code{SUERef} values to their type definitions. This implies, that
tagless types may be entered in different symbol tables under different keys. This must be detected and handled
when combining the symbol tables.

As described in Section~\ref{impl-ccode-read}, Gencot uses for its processing the list of \code{DeclEvent}s which is
derived from the \code{GlobalDecls} map. This means, the combination could be implemented on the \code{GlobalDecls}
maps or even on the \code{DeclEvent} lists. However, as described in Section~\ref{impl-ccode-trav}, Gencot also
uses the symbol table during processing, for looking up identifiers. For this reason the combination is implemented
on the symbol tables.

A language-c symbol table is implemented by the type 
\begin{verbatim}
  data DefTable = DefTable {
    identDecls   :: NameSpaceMap Ident IdentEntry,
    tagDecls   :: NameSpaceMap SUERef TagEntry,
    labelDefs  :: NameSpaceMap Ident Ident,
    memberDecls :: NameSpaceMap Ident MemberDecl,
    refTable   :: IntMap Name,
    typeTable  :: IntMap Type
  }
\end{verbatim}
where a \code{NameSpaceMap k v} is a mapping from \code{k} to \code{v} with nested scopes. The last two components are the 
relation caches as described above, they are ignored and combined to be empty. After the analysis phase, on the toplevel, 
the maps \code{labelDefs} and \code{memberDecls} are empty, since in C there are no global labels and \code{memberDecls}
contains the struct/union members only while processing the corresponding declaration. So only the first two maps must
be combined.

The map \code{identDecls} contains all identifiers with file scope. For them, the linkage is relevant. If an identifier has 
internal linkage, it is only valid in its symbol table and may denote a different object in another symbol table. These
identifiers are not relevant for Gencot when processing several source files together, thus they are omitted when the
symbol tables are combined. Only identifiers with external or no linkage are retained in the combined symbol table. Note
that this implies that identifiers with internal linkage cannot be looked up in the combined table anymore.

If an identifier has external linkage, it is assumed to denote the same object in every symbol table. However, it may be
declared in one symbol table and defined in another one. In this case, always the definition is used for the combined
table, the declaration is ignored. Only if it is declared in both symbol tables one of the declarations is put into the
combined table. Note that if the C program is correct, the identifier may be defined in at most one C compilation unit
and thus a definition for it occurs in at most one of the symbol tables. 

If both symbol tables contain a definition for the same identifier with external linkage Gencot compares their source 
file positions. If both positions are the same, the definition is in a file included by both sources and it is correct to
discard one of them. If the positions are not the same, Gencot signals an error. The typical case for such included 
definitions are object declarations where the \code{extern} specifier is omitted. This is interpreted as a ``tentative''
definition in C and is classified as definition by the language-c analysis step, if it is not followed by a definition.

The dummy declarations generated by Gencot for parameterless macro definitions (see Section~\ref{impl-ccode-dummydecl})
have the form of such tentative definitions. Since they are generated separately for every C source file, they cannot
be recognized by comparing their source file positions. To prevent them from being signaled as duplicate definitions they
are explicitly specified as \code{extern}, then they are classified as declarations by language-c (instead as tentative 
definitions) and no error is signaled by Gencot. Alternatively they could be generated with internal linkage (specified 
as \code{static}), then they are removed before combining the symbol tables. However, then their names are mapped differently,
as described in Section~\ref{design-names}. Note that (manually created) dummy declarations for
macros with parameters are never classified as definitions, since they have the form of a C function declaration without
a body. 

The typical cases for toplevel identifiers with no linkage are typedef names and struct/union/enum tags. Such an identifier
may occur in two symbol tables, if it is
defined in a file included by both corresponding \code{.c} files. In this case it names the same type and one of both
entries is put in the combined table. However, it may also be the case that the identifier is defined in both \code{.c}
files and used for different types. In this case the combination approach does not work. Gencot assumes that this case
does not occur and tests whether the identifier is mapped to the same semantics (determined from the position information
of the corresponding definition). If not, an error is signaled, this must be handled manually by the developer.

The map \code{tagDecls} contains all tags of struct/union/enum types, mapped to the type definition. For tagless types
the unique identifier is used as key, as described above. If a tag is present, it is treated in the same way as other 
identifiers with no linkage.

A tagless struct/union/enum type in C can be referenced only from a single place, since it must be syntactically embedded
there. This means, when the same tagless type occurs in two symbol tables using a different key, every symbol table
contains at most one reference to the key. When the symbol tables are combined, at most one of these references are transferred
to the combined table. Thus it is possible to use the same key for the transferred definition to yield a consistent
combined table. The simples way to do so would be to always use the entry of the same symbol table for the combination 
when an object occurs in both. However, this is not possible, since for identifiers with external linkage the definition
must be preferred over a declaration, independent where it occurs. Therefore Gencot first transfers both definitions to the 
combined table and afterwards removes all definitions which are not referenced there.

Finally, it may happen that the same unique identifier is used in different symbol tables to reference different types, 
as described for type names above. This is not a problem in the C sources, it is an internal collision of the parser-generated 
unique identifiers. The easiest way to solve this is to introduce a tag for at least one of both tagless types.

The combination is implemented by the function
\begin{verbatim}
  combineTables :: DefTable -> DefTable -> DefTable
\end{verbatim}
in module \code{Gencot.Package}. It should be applied to the tables built by \code{readFromFile} for two different
\code{.c} files of the same package. It can be iterated to combine the result of \code{readPackageFromInput}.
The result can then be processed mainly in the same way as described in 
Section~\ref{impl-ccode-read} for a table built from a single input file.

\subsection{Dummy Declarations for Preprocessor Macros}
\label{impl-ccode-dummydecl}

As described in Section~\ref{design-preprocessor-macros} macro calls in C code must either be syntactically correct
C code or they must be converted to syntactically correct C code. Due to the language-c analysis step this is not 
sufficient. The analysis step checks for additional properties. In particular, it requires that every identifier 
is either declared or defined.

Thus for every identifier which is part of a converted macro call a corresponding declaration must be added to the 
C code. They are called ``dummy declarations'' since they are only used for making the analysis step happy. 

For all preprocessor defined constants Gencot automatically generates the required dummy declarations. The corresponding
macro calls always have the form of a single identifier occurring at positions where a C expression is expected. The type
of the identifier is irrelevant, hence Gencot always uses type \code{int} for the dummy declarations. For every preprocessor
constant definition of the form 
\begin{verbatim}
  #define NNN XXX
\end{verbatim}
a dummy declaration of the form
\begin{verbatim}
  static int NNN;
\end{verbatim}
is generated. This is implemented by the additional filter \code{gencot-gendummydecls}. It is applied to the result of 
\code{gencot-selppconst}. The resulting dummy declarations are prepended to the input of the language-c preprocessor
since this prevents the lines from being counted for the \code{<stdin>} part.

Dummy declarations are generated with internal linkage. This causes them to be removed when symbol tables are combined.
Otherwise, since they always declare objects and are classified by language-c as (tentative) definitions, they would
be signaled as repeated definitions when they occur in several symbol tables (see Section~\ref{impl-ccode-package}).

Flag macro calls do not occur in C code, hence no dummy declarations are required for them.

For all other macros the required dummy declarations must be created manually and added to the Gencot macro call conversion.
Even if no macro call conversion is needed because the macro calls are already in C syntax, it may be necessary to
add dummy declarations to satisfy the requirements of the language-c analysis step.

\subsection{Generating Cogent Code}
\label{impl-ccode-gencog}

When Gencot generates its Cogent target code it uses the data structures defined by the Cogent compiler for representing
its AST after parsing Cogent code. The motivation to do so is twofold. First, the AST omits details such as using code layout
and parentheses for correct code structure and the Cogent compiler provides a prettyprint function for its AST which cares
about these details. Hence, it is much easier to generate the AST and use the prettyprinter for output, instead of generating
the final Cogent program text. Second, by using the Cogent AST the generated Cogent code is guaranteed to be syntactically correct and
current for the Cogent language version of the used compiler version. Whenever the Cogent language syntax is changed
in a newer version, this will be detected when Gencot is linked to the newer compiler version.

\subsubsection{Cogent Surface Syntax Tree}

The data structures for the Cogent surface syntax AST are defined in the module Cogent.Surface. It defines parameterized types
for the main Cogent syntax constructs (\code{TopLevel}, \code{Alt}, \code{Type}, \code{Polytype}, \code{Pattern}, 
\code{IrrefutablePattern}, \code{Expr}, and \code{Binding}), where the type parameters determine the types of the 
sub-structures. Hence the AST types
can easily be extended by wrapping the existing types in own extensions which are then also used as actual type parameters.

Cogent itself defines two such wrapper type families: The basic unextended types \code{RawXXX} and the types \code{LocXXX}
where every construct is extended by a representation of its source location. 

All parameterized types for syntax constructs and the \code{RawXXX} and \code{LocXXX} types are defined as instances of 
class \code{Pretty} from
module \code{Text.PrettyPrint.ANSI.Leijen}. This prettyprinter functionality is used by the Cogent compiler for outputting
the parsed Cogent source code after some processing steps, if requested by the user.

As source location representation in the \code{LocXXX} types Cogent uses the type \code{SourcePos} from Module 
\code{Text.Parsec.Pos} in package \code{parsec}.
It contains a file name and a row and column number. This information is ignored by the prettyprinter.

\subsubsection{Extending the Cogent Surface Syntax}

Gencot needs to extend the Cogent surface syntax for its generated code in two ways:
\begin{itemize}
\item origin markers must be supported, as described in Section~\ref{impl-origin},
\item C function bodies must be supported in Cogent function definitions, as described in Section~\ref{design-fundefs-body}.
\end{itemize}

\paragraph{Origin Markers}

The origin markers are used to optionally surround the generated target code parts, which may be arbitrary syntactic constructs
or groups of them. Hence it would be necessary to massively extend the Cogent surface syntax, if they are added as explicit 
syntactic constructs. Instead, Gencot optionally adds the information about the range of source lines to the syntactic
constructs in the AST and generates the actual origin markers when the AST is output. 

Although the \code{LocXXX} types already support a source position in every syntactic construct, it cannot be used by Gencot,
since it represents only a single position instead of a line range. Gencot uses the \code{NodeInfo} values, since they represent
a line range and they are already present in the C source code AST, as described in Section~\ref{impl-ccode-read}. Hence, they
can simply be transferred from the source code part to the corresponding target code part. For the case that there is no
source code part in the input file (such as for code generated for external name references), or there is no position 
information available for the source code part, the \code{NodeInfo} is optional.

It may be the case that a target AST node is generated from a source code part which is not a single source AST node. Then
there is no single \code{NodeInfo} to represent the origin markers for the target AST node. Instead, Gencot uses the 
\code{NodeInfo} values of the first and last AST nodes in the source code part.

It may also be the case that a structured source code part is translated to a sequence of sub-part translations without target
code for the main part. In this case the \code{\#ORIGIN} marker for the main part must be added before the \code{\#ORIGIN} 
marker of the first target code part and the \code{\#ENDORIG} marker for the main part must be added after the \code{\#ENDORIG} 
marker of the last target code part. 

To represent all these cases, the origin information for a construct in the target AST consists of two lists of \code{NodeInfo}
values. The first list represents the sequence of \code{\#ORIGIN} markers to be inserted before the construct, here only the
start line numbers in the \code{NodeInfo} values are used. The second list represents the sequence of \code{\#ENDORIG} markers 
to be inserted after the construct, here only the end line numbers in the \code{NodeInfo} values are used. If no marker of
one of the kinds shall be present, the corresponding list is empty.

Additional information must be added to represent the marker extensions for placing the comments (the trailing ``+'' signs).
Therefore, a boolean value is added to all list elements.

Together, Gencot defines the type \code{Origin} for representing the origin information, with the value \code{noOrigin}
for the case that no markers will be generated:
\begin{verbatim}
  data Origin = Origin { 
    sOfOrig :: [(NodeInfo,Bool)], 
    eOfOrig :: [(NodeInfo,Bool)] } 
  noOrigin = Origin [] []
\end{verbatim}
Gencot adds an \code{Origin} value to every Cogent AST element. The type \code{Origin} is defined in the module 
\code{Gencot.Origin}

\paragraph{Embedded C Code}

Cogent function definitions are represented by the \code{FunDef} alternative of the type for toplevel syntactic constructs:

\begin{verbatim}
  data TopLevel t p e = 
    ... | FunDef VarName (Polytype t) [Alt p e] | ...
\end{verbatim}
The type parameter \code{e} for representing syntactic expressions is only used in this alternative and in the alternative
for constant definitions. Cogent constant definitions are generated by Gencot only from C enum constants (preprocessor
constants are processed by \code{gencot-prcconst} which is not implemented in Haskell). The defined value for a C enum
constant is represented in the C AST by the type for expressions. Together, instead of Cogent expressions, Gencot always
uses either a C expression or a Cogent expression together with a C function body (which syntactically is a statement) 
in the Cogent AST. 

To modify the Cogent syntax in this way, Gencot defines an own expression type with two alternatives for a C expression 
and for a Cogent expression together with a C statement:
\begin{verbatim}
  data GenExpr = ConstExpr Exp
               | FunBody RawExpr Stm
\end{verbatim}
where \code{Exp} and \code{Stm} are the types for C expressions and statements as defined by the language-c-quote AST 
(see Section~\ref{impl-ccode-expr}). Note that no \code{Origin} components are added, since the types \code{Exp} and 
\code{Stm} already contain \code{Origin} information. The type \code{RawExpr} is used for the dummy result expression.
It has no origin in the C source, therefore the raw type without origin information is sufficient. 

Other than for the dummy result expression, the Cogent AST expression type is not used by Gencot. 
Since bindings only occur in expressions, the AST type for Cogent bindings is not used either.

For the type parameters \code{t} and \code{p} for representing types and patterns, respectively, the normal types for 
the Cogent constructs are used, since Gencot generates both in Cogent syntax. The pattern generated for a function
definition is always a tuple pattern, which is irrefutable. Gencot never generates other patterns, hence the AST
type for irrefutable patterns is sufficient. 

Together, Gencot uses the following types to represent its extended Cogent surface AST:
\begin{verbatim}
  data GenToplv =
    GenToplv Origin (TopLevel GenType GenIrrefPatn GenExpr)
  data GenAlt =
    GenAlt Origin (Alt GenIrrefPatn GenExpr)
  data GenIrrefPatn = 
    GenIrrefPatn Origin (IrrefutablePattern VarName GenIrrefPatn)
  data GenType = 
    GenType Origin (Type GenExpr GenType)
  data GenPolytype = 
    GenPolytype Origin (Polytype GenType)
\end{verbatim}
The first parameter of \code{Type} for expressions is only used for Cogent array types, which are currently 
not generated by Gencot.

All five wrapper types are defined as instances of class \code{Pretty}, basically by applying the Cogent prettyprint
functionality to the wrapped Cogent AST type.

\subsection{Mapping Names}
\label{impl-ccode-names}

Names used in the target code are either mapped from a C identifier or introduced, as described in 
Section~\ref{design-names}. Different schemas are used depending on the kind of name to be generated.
The schemas require different information as input.

\subsubsection{General Name Mapping}

The general mapping scheme is applied whenever a Cogent name is generated from an existing C identifier.
Its purpose is to adjust the case, if necessary and to avoid conflicts between the Cogent name and
the C identifier.

As input this scheme only needs the C identifier and the required case for the Cogent name.
It is implemented by the function
\begin{verbatim}
  mapName :: Bool -> Ident -> String
\end{verbatim}
where the first argument specifies whether the name must be uppercase.

\subsubsection{Cogent Type Names}

A Cogent type name (including the names of primitive types) may be generated as translation of a C 
primitive type, a C typedef name, or a C struct/union/enum type reference. 

A C primitive type is translated according to the description in Section~\ref{design-types}. Only the
type specifiers for the C type are required for that.

A C typedef name is translated by simply mapping it with the help of \code{mapName} to an uppercase name.
Only the C typedef name is required for that.

A C struct/union/enum type reference may be tagged or tagless. If it is tagged, the Cogent type name is
constructed from the tag as described in Section~\ref{design-names}: the tag is mapped with the help of
\code{mapName} to an uppercase name, then a prefix \code{Struct\_}, \code{Union\_} or \code{Enum\_} is 
prepended. For this mapping the tag and the kind (struct/union/enum) are required. Both are contained
in the language-c type \code{TypeName} which is used to represent a reference to a struct/union/enum.

If the reference is untagged, Gencot nevertheless generates a type name, as motivated and described 
in Section~\ref{design-names}. As input it needs the kind and the position of the struct/union/enum 
definition. The latter is not contained in the \code{TypeName}, it contains the position of the reference
itself. To access the position of the definition, the definition must be retrieved from the symbol table
in the monadic state. To access the real name of the input file it must be retrieved from the user
state (see Section~\ref{impl-ccode-trav}). Hence, the mapping function is defined as a monadic action. 

Together the function for translating struct/union/enum type references is
\begin{verbatim}
  transTagName :: TypeName -> f String
\end{verbatim}
where \code{f} is a \code{FileNameTrav} and \code{MonadTrav} (see Section~\ref{impl-ccode-trav}).

If the definition itself is translated, it is already available and need not be retrieved from the map. 
However, the user state is still needed to map the generic name \code{<stdin>} to the true source file 
name. Therefore Gencot uses function \code{transTagName} also when translating the definition.

\subsubsection{Cogent Function Names}

Cogent function names are generated from C function names. A C function may have external or internal
linkage, according to the linkage the Cogent name is constructed either as a global name or as a name specific
to the file where the function is defined. For deciding which variant to use for a function name reference,
its linkage must be determined. It is available in the definition or in a declaration for the function name,
either of which must be present in the symbol table. The language-c analysis module replaces all 
declarations in the tyble by the
definition, if that is present in the parsed input, otherwise it retains a declaration. 

A global function name is generated by mapping the C function name with the help of \code{mapName} to
a lowercase Cogent name. No additional information is required for that.

For generating a file specific function name, the file name of the definition is required. Note that 
this is only done for a function with internal linkage, where the definitions must be present in
the input whenever the function is referenced. The definition contains the position information
which includes the file name. Hence, the symbol table together with the real name of the input file 
is sufficient for translating the name. To make both available the translation function is defined as 
a monadic action.

In C bodies function names cannot be syntactically distinguished from variable names. Therefore, Gencot
uses a common function for translating function and variable names. For a description how variable
names are translated see Section~\ref{impl-ccode-expr}.
\begin{verbatim}
  transObjName :: Ident -> f String
\end{verbatim}
where \code{f} is a \code{FileNameTrav} and \code{MonadTrav} (see Section~\ref{impl-ccode-trav}).

Similar as for tags, the function is also used when translating a function definition, although the 
definition is already available.

\subsubsection{Cogent Constant Names}

Cogent constant names are only generated from C enum constant names. They are simply translated
with the help of \code{mapName} to a lowercase Cogent name. No additional information is required.

\subsubsection{Cogent Field Names}

C member names and parameter names are translated to Cogent field names. Only if the C name is
uppercase, the name is mapped to a lowercase Cogent name with the help of \code{mapName}, 
otherwise it is used without change. Only the C name is required for that, in both cases it is
available as a value of type \code{Ident}. The translation is implemented by the function
\begin{verbatim}
  mapIfUpper :: Ident -> String
\end{verbatim}

\subsection{Generating Origin Markers}
\label{impl-ccode-origin}

For outputting origin markers in the target code, the AST prettyprint functionality must be extended.

The class \code{Pretty} used by the Cogent prettyprinter defines the methods
\begin{verbatim}
  pretty :: a -> Doc
  prettyList :: [a] -> Doc
\end{verbatim}
but the method \code{prettyList} is not used by Cogent. Hence, only the method \code{pretty} needs to be defined
for instances. The type \code{Doc} is that from module \code{Text.PrettyPrint.ANSI.Leijen}.

The basic approach is to wrap every syntactic construct in a sequence of \code{\#ORIGIN} markers and 
a sequence of \code{\#ENDORIG} markers according to the origin information for the construct in the extended AST. 
This is done by an instance definition of the form
\begin{verbatim}
  instance Pretty GenToplv where
    pretty (GenToplv org t) = addOrig org $ pretty t
\end{verbatim}
for \code{GenToplv} and analogous for the other types. The function \code{addOrig} has the type
\begin{verbatim}
  addOrig :: Origin -> Doc -> Doc
\end{verbatim}
and wraps its second argument in the origin markers according to its first argument.

The Cogent prettyprinter uses indentation for subexpressions. Indentation is implemented by the \code{Doc} type, 
where it is called ``nesting''. The prettyprinter maintains a
current nesting level and inserts that amount of spaces whenever a new line starts. 

The origin markers must be positioned in a separate line, hence \code{addOrig} outputs a newline before and after
each marker. This is done even at the beginning of a line, since due to indentation it cannot safely be determined
whether the current position is at the beginning of a line. Cogent may change the nesting of the next line after \code{addOrig}
has output a marker (typically after an \code{\#ENDORIG} marker). The newline at the end of the previous marker 
still inserts spaces according to the old nesting level, which determines the current position at the begin of
the following marker. This is not related to the new nesting level. 

This way many additional newlines are generated, in
particular an empty line is inserted between all consecutive origin markers. The additional newlines are later removed
together with the markers, when the markers are processed. Note that, if a syntactic construct
is nested, the indentation also applies to the origin markers and the line after it. To completely remove an
origin marker from the target code it must be removed together with the newline before it and with the newline 
after it and the following indentation. The following indentation can be determined since it is the same as that 
for the marker itself (a sequence of blanks of the same length). 

\subsubsection{Repeated Origin Markers}

Normally, target code is positioned in the same order as the corresponding source code. This implies, that
origin markers are monotonic. A repeated origin marker is a marker with the same line number as its previous marker.
Repeated origin markers of the same kind must be avoided, since they would result in duplicated comments or 
misplaced directives.
Repeated origin markers of the same kind occur, if a subpart of a structured source code part begins or ends 
in the same line as its main part. In this case only the outermost markers must be retained.

An \code{\#ENDORIG} marker repeating an \code{\#ORIGIN} marker means that the source code
part occupies only one single line (or a part of it), this is a valid case. 
An \code{\#ORIGIN} marker repeating an \code{\#ENDORIG} marker means that the previous source code
part ends in the same line where the following source code part begins. In this case the markers are
irrelevant, since no comments or directives can be associated with them. However, if they are
present they introduce unwanted line breaks, hence they also are avoided by removing both of them.

Together, the following rules result. In a sequence of repeated \code{\#ORIGIN} markers, only the first one 
is generated. In a sequence of repeated \code{\#ENDORIG} markers only the last one is generated.
If an \code{\#ORIGIN} marker repeats an \code{\#ENDORIG} marker, both are omitted.

There are several possible approaches for omitting repeated origin markers:
\begin{itemize}
\item omit repeated markers when building the Cogent AST
\item traverse the Cogent AST and remove markers to be omitted
\item output repeated markers and remove them in a postprocessing step
\end{itemize}
Note, that it is not possible to remove repeated markers already in the language-c AST, since there a \code{NodeInfo}
value always corresponds to two combined markers.

Handling repeated markers in the Cogent AST is difficult, because for an \code{\#ORIGIN} marker the context
before it is relevant whereas for an \code{\#ENDORIG} marker the context after it is relevant. An additional
AST traversal would be required to determine the context information. The first approach is even more complex
since the context information must be determined from the source code AST where the origin markers are not
yet present. 

For this reason Gencot uses the third approach and processes repeated markers in the generated target code text,
independent from the syntactical structure.

\subsubsection{Filter for Repeated Origin Marker Elimination}

The filter \code{gencot-reporigs} is used for removing repeated origin markers. It is implemented as an awk script.

It uses five string buffers: two for the previous two origin markers read, and three for the code before,
between, and after both markers. Whenever all buffers are filled (the buffer after both markers with a 
single text line; this line exists, since consecutive markers are always separated by an empty line),
the markers are processed as follows, if they have the same line number: in the case of two 
\code{\#ORIGIN} markers the second is deleted, in the
case of two \code{\#ENDORIG} markers the first is deleted, and in the case of an \code{\#ORIGIN}
marker after an \code{\#ENDORIG} marker both are deleted. In the latter case the line number of the
\code{\#ORIGIN} marker is remembered and subsequent \code{\#ORIGIN} markers with the same line number
are also deleted.

When both markers have different line numbers or if an \code{\#ENDORIG} marker follows an \code{\#ORIGIN}
marker the first marker and the code before it are output and the buffers are filled until the next marker
has been read.

\subsection{Generating Expressions}
\label{impl-ccode-expr}

For outputting the Cogent AST the prettyprint functionality must be extended to 
output C function bodies and the C expressions used for constant definitions. Additionally, at least in function bodies,
origin markers must be generated to be able to re-insert comments and preprocessor directives. Finally, all names
occurring free in a function body or a constant expression must be mapped to Cogent names.

The language-c prettyprinter is defined in module \code{Language.C.Pretty}. It defines an own class \code{Pretty} with 
method \code{pretty} to convert the AST types to a \code{Doc}. However, other than the Cogent prettyprinter, it uses 
the type \code{Doc} from module \code{Text.PrettyPrint.HughesPJ} instead of module \code{Text.PrettyPrint.ANSI.Leijen}.
This could be adapted by rendering the \code{Doc} as a string and then prettyprinting this string to a \code{Doc}
from the latter module. This way, a prettyprinted function body could be inserted in the document created by the
Cogent prettyprinter.

\subsubsection{Origin Markers}

For generating origin markers, a similar approach is not possible, since they must be inserted between single statements,
hence, the function \code{pretty} must be extended. Although it does not use the \code{NodeInfo}, it is only defined for
the AST type instances with a \code{NodeInfo} parameter and has no genericity which could be exploited for extending it.
Therefore, Gencot has to fully reimplement it. 

In the prettyprint reimplementation the target code parts must be wrapped by origin markers
in the same way as for the Cogent AST. However, for the type \code{Doc} from module \code{Text.PrettyPrint.HughesPJ} 
this is not possible, since newlines are only
available as separators between documents and cannot be inserted before or after a document. An alternative choice
would be to use the type \code{Doc} from \code{Text.PrettyPrint.ANSI.Leijen}, as the Cogent prettyprinter does.
However, the approach of both modules is quite different so that it would be necessary to write a new C 
prettyprint implementation nearly from scratch. 

It has been decided to use another approach which is expected to be simpler. The alternative C parser language-c-quote 
also has a prettyprinter. It generates a type \code{Doc} defined by a third module \code{Text.PrettyPrint.Mainland}.
It is similar to \code{Text.PrettyPrint.ANSI.Leijen} and also supports adding newlines before and after a document.
The language-c-quote prettyprinter is defined in the module \code{Language.C.Pretty} of language-c-quote and consists
of the method \code{ppr} of the class \code{Pretty} defined in module \code{Text.PrettyPrint.Mainland.Class.Pretty}.
This method is not generic at all, hence it must be completely reimplemented to extend it for generating origin 
markers. However, this reimplementation is straightforward and can be done by copying the original implementation
and only adding the origin marker wrappings. The resulting Gencot module is \code{Gencot.C.Output}.

Whereas the type \code{Doc} from \code{Text.PrettyPrint.ANSI.Leijen} provides a \code{hardline} document which always
causes a newline in the output, the type \code{Doc} from \code{Text.PrettyPrint.Mainland} does not. Normal line breaks
are ignored in certain contexts, if there is enough room. Using normal line breaks around origin markers could result
in origin markers with other code in the same line before or after the marker.

For the reimplemented language-c-quote prettyprinter Gencot defines its own \code{hardline} by using a newline 
which is hidden for type \code{Doc}. This could be implemented without nesting the marker and the subsequent line.
However, if at the marker position a comment is inserted, the subsequent line should be correctly indented.
To achieve this, the \code{hardline} implementation also adds the current nesting after the newline.

Hiding the newline from \code{Doc} implies that the ``current column'' maintained by \code{Doc} is not
correct anymore, since it is not reset by the \code{hardline}. Every \code{hardline} will instead advance the current
column by the width of the marker and twice the current nesting. This has two consequences.

First, in some places the language-c-quote prettyprinter uses ``alignment'' which means an indentation of subsequent lines
to the current column. This indentation will be too large after inserted markers. Gencot handles this by replacing 
alignment everywhere in the prettyprint implementation by a nesting of two additional columns. 

Second, the language-c-quote prettyprinter is parameterized by a ``document width''. It automatically breaks lines 
when the current column exceeds the document width. The incorrect column calculation causes many additional such
line breaks, since the current column increases much faster than normal. Gencot handles this by setting the document
width to a very large value (such as 2000 instead of 80) to compensate for the fast column increase.

\subsubsection{Using the language-c-quote AST}

Language-c-quote uses a different C AST implementation than language-c. To use its reimplemented prettyprinter, the 
language-c AST must be translated to a language-c-quote AST. This is not trivial, since the structures are somewhat
different, but it seems to be simpler than implementing a new C prettyprinter. The translation is implemented in
the module \code{Gencot.C.Translate}. 

Additionally the language-c-quote AST must be extended by \code{Origin} values. The language-c-quote AST already 
contains \code{SrcLoc} values which are similar to the \code{NodeInfo} values in language-c. Like these they cannot
be used as origin marker information since they cannot represent begin and end markers independently. Therefore
Gencot also reimplements the language-c-quote AST by copying its data types and replacing the \code{SrcLoc}
values by \code{Origin} values. This is implemented in module \code{Gencot.C.Ast}.

Together, this approach yields a similar structure as for the translation to Cogent: The Cogent AST is extended 
by the structures in \code{Gencot.C.Ast} to represent function bodies and constant expressions. The function for
translating from language-c AST to the Cogent AST is extended by the functions in \code{Gencot.C.Translate} to
translate function bodies and constant expressions from the language-c AST to the reimplemented language-c-quote 
AST, and the Cogent prettyprinter is extended by the prettyprinter
in \code{Gencot.C.Output} to print function bodies and constant expressions with origin markers.

In addition to translating the C AST structures from language-c to those of language-c-quote, the translation
function in \code{Gencot.C.Translate} implements the following functionality:
\begin{itemize}
\item generate \code{Origin} values from \code{NodeInfo} values,
\item map C names to Cogent names.
\end{itemize}

\subsubsection{Name Mapping}

Name mapping depends on the kind of name and may additionally depend on its type. Both information is
available in the symbol table (see Section~\ref{impl-ccode-trav}). However, the scope cannot be queried
from the symbol table. Hence it is not possible to map names depending on whether they are locally defined
or globally.

The following kinds of names may occur in a function body: primitive types, typedefs, tags, members, 
functions, global variables, enum constants, preprocessor constants, parameters and local variables.

Primitive type names and typedef names can only occur as name of a base type in a declaration. Primitive
type names are mapped to Cogent primitive type names as described in Section~\ref{design-types-prim}.

A typedef name may also occur in a declarator of a local typedef which defines the name. 
In both cases, as described in Section~\ref{design-fundefs-body}, Gencot
only maps the plain typedef names, not the derived types. The typedef names are mapped according to
Section~\ref{design-types-typedef}: If they ultimately resolve to a struct, union, or array type they
are mapped with an unbox operator applied, otherwise they are mapped without.

A tag name can only occur as base type in a declaration. It is always mapped to a name with a prefix 
of \code{Struct\_}, \code{Union\_}, or \code{Enum\_}. Tagless structs/unions/enums are not mapped at all.
Tag names are mapped according to Sections~\ref{design-types-enum} and~\ref{design-types-struct}: struct
and union tags are mapped with an unbox operator applied, enum tags are mapped without.

Gencot also maps defining tag occurrences. Thus an occurrence of the form 
\begin{verbatim}
  struct s { ... }
\end{verbatim}
is translated to
\begin{verbatim}
  struct #Struct_s { ... }
\end{verbatim}

Every occurrence of a field name can be syntactically distinguished. It is mapped according to 
Section~\ref{design-names} to a lowercase Cogent name if it is uppercase, otherwise it is unchanged.
Field names are also mapped in member declarations in locally defined structures and unions.

All other names syntactically occur as a primary expression. They are mapped depending on their semantic
information retrieved from the symbol table. In a first step it distinguishes objects, functions, 
and enumerators.

An object identifier may be a global variable, parameter, or local variable. It may also be a preprocessor 
constant since for them dummy declarations have been introduced which makes them appear as a global variable
for the C analysis. For the mapping the linkage is relevant, this is also available from the symbol table.

Identifiers for global variables may have external or internal linkage and are mapped depending on the
linkage. Identifiers for parameters always have no linkage and are always mapped like field names. Identifiers
for local variables either have no linkage or external linkage. In the first case they are mapped like
field names. In the second case they cannot be distinguished from global variables with external linkage,
and are mapped to lowercase. The dummy declarations introduced for preprocessor constants
always have external linkage, the identifiers are mapped to lowercase. Together, object identifiers with 
internal linkage are mapped as described in Section~\ref{design-names}, object identifiers with external
linkage are mapped to lowercase, and object identifiers with no linkage are mapped to lowercase if they are
uppercase and remain unchanged otherwise.

An identifier for a function has either internal or external linkage and is mapped depending on its linkage.
An identifier for an enumerator is always mapped to lowercase, like preprocessor constants.

Identifiers for local variables may also occur in a declarator of a local object definition which defines 
the name. They are also mapped depending on their linkage, as described above.

\subsection{Traversing the C AST}
\label{impl-ccode-trav}

The package language-c uses a monad for traversing and analysing the C AST. The monad is defined in module 
\code{Language.C.Analysis.TravMonad} and mainly provides the symbol table and user state during the traversal.
The traversal itself is implemented by a recursive descent according to the C AST using a separate function
for analysing every syntactic construct. 

When processing the semantic map resulting from the language-c analysis Gencot implements similar recursive 
descents using a processing function for every syntactic construct. For this it uses the same monad for two reasons.
\begin{itemize}
\item the definitions and declarations of the global identifiers are needed for accessing their types and for
mapping the identifiers to Cogent names,
\item additionally, the definitions and declarations of locally defined identifiers are needed in C function
bodies for the same purpose.
\end{itemize}

The global definitions and declarations in the symbol table correspond to the semantics map which is the result 
of the language-c analysis step. It is created from the symbol table after the initial traversal of the C AST. Although Gencot 
processes the content of the semantics map, it is not available as a whole in the processing functions. Instead
of passing the semantics map as an explicit parameter to all processing functions, Gencot uses monadic traversals
through the relevant parts of the semantics map, which implicitly make the symbol table available to all 
processing functions. This is achieved by reusing the symbol table after the analysis phase for the traversals
of the semantics map.

Additionally, when processing the C function bodies, the symbol table is used for managing the local declarations. 
This is possible because although the analysis phase translates global declarations and definitions to a 
semantic representation, it does not modify function bodies and returns them as the original C AST. Since
the information about local declarations is discarded at the end of its scope, the information is not 
present anymore in the symbol table after the analysis phase. Gencot uses the symbol table functionality
to rebuild this information during its own traversals.

The user state is used by Gencot to provide additional information, depending on the purpose of the traversal.
A common case is to make the actual name of the processed file available during processing. In the \code{NodeInfo}
values in the AST it is always specified as \code{<stdin>} since the input is read from a pipe. All C processing 
filters take the name of the original C source file as an additional argument. It is added to the user state 
of traversal monads so that it can be used during traversal.

This is supported by defining in module \code{Gencot.Name} the class \code{FileNameTrav} as
\begin{verbatim}
  class (Monad m) => FileNameTrav m where
    getFileName :: m String
\end{verbatim}
so that the method \code{getFileName} can be used to retrieve the source file name from all traversal monads of 
this class. Based on this the monadic action \code{srcFileName} is defined to return the file name for a 
\code{NodeInfo} value and replace it by the original source file name if it is equal to \code{<stdin>}.

Another information needed during C AST traversal is the parameter modification description (see Section~\ref{impl-parmod}).
It is used for translating the types of all functions, as described in Section~\ref{design-types-function}.

Finally, Gencot maintains a list of tag definitions which are processed in advance as nested, as described in 
Section~\ref{impl-ccode-read}. This list is implemented as a list of elements of type \code{SUERef} which is used
by language-c to identify both tagged and untagged struct/union/enum types.

The utilities for the monadic traversal of the semantics map are defined in module \code{Gencot.Traversal}. 
The main monadic type is defined as
\begin{verbatim}
  type FTrav = Trav (String,ParmodMap,[SUERef])
\end{verbatim}
where \code{String} is the type used for storing the original C source file name in the user state,
\code{ParmodMap} is the type for storing the parameter modification description, and the third component is the list for
maintaining tag definitions processed as nested. \code{FTrav} is an
instance of \code{FileNameTrav}. As execution function for the monadic actions the functions
\begin{verbatim}
  runFTrav :: DefTable -> (String,ParmodMap) -> FTrav a -> IO a
  runWithTable :: DefTable -> FTrav a -> IO a
\end{verbatim}
are defined. The first one takes the symbol table, the original C source file name and the parameter modification
description as arguments to initialize the state. The tag definition list is always initialized as empty. The second 
function leaves also the other two components empty. The functions are themselves 
\code{IO} actions and print error messages generated during traversal to the standard output.

In the monadic actions the symbol table can be accessed by actions defined in the modules 
\code{Language.C.Analysis.TravMonad} and \code{Language.C.Analysis.DefTable}. An identifier can be
resolved using the actions
\begin{verbatim}
  lookupTypeDef :: Ident -> FTrav Type
  lookupObject :: Ident -> FTrav (Maybe IdentDecl)
\end{verbatim}
For resolving tag definitions the symbol table must be retrieved by
\begin{verbatim}
  getDefTable :: FTrav DefTable
\end{verbatim}
then the struct/union/enum reference can be resolved by
\begin{verbatim}
  lookupTag :: SUERef -> DefTable -> Maybe TagEntry
\end{verbatim}
To maintain the list of tag definitions processed as nested two monadic actions are defined:
\begin{verbatim}
  markTagAsNested :: SUERef -> FTrav ()
  isMarkedAsNested :: SUERef -> FTrav Bool
\end{verbatim}

Additionally, there are actions to enter and leave a scope and actions for inserting definitions.

An error can be recorded in the monad using the action
\begin{verbatim}
  recordError :: Language.C.Data.Error.Error e => e -> m () 
\end{verbatim}

The parameter modification description can be accessed by the monadic action
\begin{verbatim}
  getParmods :: String -> FTrav [String]
\end{verbatim}
where the \code{String} argument is a function identifier and the result is the list of parameter description values
for the parameters of the identified function.

\subsection{Creating and Using the C Call Graph}
\label{impl-ccode-callgraph}

In some Gencot components we use the C call graph. This is the mapping from functions to the functions
invoked in their body. Here we describe the module \code{Gencot.Util.CallGraph} which provides
utility functions for creating and using the call graph.

The set of invoked functions is determined by traversing the bodies of all function definitions after the analysis
phase. The callback handler is not used since it is only invoked for declarations and definitions and does not help
for processing function invocations.

Invocations can be identified purely syntactically as C function call expressions. The invoked function is usually 
specified by an identifier, however, it can be specified as an arbitrary C expression. We only support the cases
where the invoked function is specified as an identifier for a function or function pointer, by a chain of 
member access operations starting at an identifier for an object of struct or union type, or by an array index
expression where the array is specified as an identifier or member access chain and the element type is a function
pointer type. All other invocation where the invoked function is specified in a different way are ignored and not 
added to the call graph.

The starting identifier can be locally declared, such as a parameter of the function where the invocation occurs. The 
declaration information of these identifiers would not be available after the traversal which builds the call graph.
To make the full information about the invoked functions available, Gencot inserts the declarations into the call graph 
instead of the identifiers. In the case of a member access chain it uses the struct or union type which has the 
invoked function pointer or the indexed function pointer array as its direct member. This struct or union type
must have a declared tag, otherwise the invocation is ignored and not inserted into the call graph.

The information about such an invocation in a function body is represented by the following type:
\begin{verbatim}
  data CGInvoke =
      IdentInvoke IdentDecl Int
    | MemberTypeInvoke CompType MemberDecl Int
\end{verbatim}
The additional integer value specifies the number of actual arguments in this invocation.
Note that in a function definition
the parameters are represented in the symbol table by \code{IdentDecl}s, not by \code{ParamDecl}s. In the case
of an array element invocation the actual index is ignored, all array elements are treated in a common way.

The call graph has the form of a set of globally described invocations. These are triples consisting of the definition
of the invoking function, the invocation, and a boolean value telling whether the identifier in the case of an
\code{IdentInvoke} is locally defined in the invoking function:
\begin{verbatim}
  type CallGraph = Set CGFunInvoke
  type CGFunInvoke = (FunDef, CGInvoke, Bool)
\end{verbatim}
The equality relation for values of type \code{CGFunInvoke} is based on the location of the contained declarations
in the source file. This is correct since after the initial traversal every identifier has a unique declaration associated.

To access the declarations of locally declared identifiers, the symbol table with local declarations
must be available while building the call graph. Therefore we traverse the function bodies with the help of
the \code{FTrav} monad and \code{runWithTable} as described in Section~\ref{impl-ccode-trav}.

The call graph is constructed by the monadic action
\begin{verbatim}
  getCallGraph :: [DeclEvent] -> FTrav CallGraph
\end{verbatim}
It processes all function definitions in its argument list and ignores all other \code{DeclEvent}s.

The function 
\begin{verbatim}
  getIdentInvokes :: CallGraph -> Set LCA.IdentDecl
\end{verbatim}
returns the set of all invoked functions which are specified directly as an identifier. In particular, they include
all invoked functions which are no function pointers.

The declaration of an invoked function also tells 
whether the function or object is defined or only declared. Note that the traversal for collecting invocations is a ``second 
pass'' through the C source after the analysis phase of language-c. During analysis language-c replaces
declarations in the symbol table whenever it finds the corresponding definition.

To use the call graph the \code{CallGraph} module defines a traversal monad \code{CTrav} 
with the call graph in the user state (in addition to the own source file name). The corresponding execution function is
\begin{verbatim}
  runCTrav :: CallGraph -> DefTable -> String -> CTrav a -> IO a
\end{verbatim}
The monadic action to access the call graph is
\begin{verbatim}
  lookupCallGraph :: Ident -> CTrav CallGraph
\end{verbatim}
It takes the identifier of an invoking function as argument and returns the part of the call graph for this function,
consisting of all invocations in its body.

The monad \code{CTrav} is an instance of class \code{FileNameTrav}, so the own source file name can be accessed by
\code{getFileName} (see Section~\ref{impl-ccode-trav}).
