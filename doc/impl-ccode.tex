
Parsing and processing C code in Gencot is always implemented in Haskell, to be able to use an existing
C parser. There are at least two choices for a C parser in Haskell:
\begin{itemize}
\item the package ``language-c'' by Benedikt Huber and others,
\item the package ``language-c-quote'' by Geoffrey Mainland and others.
\end{itemize}

The Cogent compiler uses the package language-c-quote for outputting the generated C code and for parsing the antiquoted
C source files. The reason is its support for quasiquotation (embedding C code in Haskell code) and antiquotation
(embedding Haskell code in the embedded C code). The antiquotation support is used for parsing the antiquoted C sources.

Gencot performs three tasks related to C code:
\begin{itemize}
\item read the original C code to be translated,
\item generate antiquoted C code for the function wrapper implementations,
\item output normal C code for the C function bodies as placeholder in the generated Cogent function definitions.
\end{itemize}

The first task is supported by both packages: a C parser reads the source text and creates an internal abstract syntax tree (AST).
Every package uses its own data structures for representing the AST. However, the language-c package provides an additional
``analysis'' module which processes the rather complicated syntax of C declarations and returns a ``symbol map'' mapping
every globally declared identifier to its declaration or definition. Since Gencot generates a single Cogent definition for
every single globally declared identifier, this is the ideal starting point for Gencot. For this reason Gencot uses
the language-c parser for the first task.

The second task is only supported by the package language-c-quote, therefore it is used by Gencot. 

The third task is supported by both packages, since both have a prettyprint function for outputting their AST. Since the 
function bodies have been read from the input and are output with only minor modifications, it is easiest to use
the language-c prettyprinter, since language-c has been used for parsing and the body is already represented by its 
AST data structures. However, the language-c prettyprinter cannot be extended to generate the ORIGIN markers, therefore
the AST is translated to the language-c-quote AST and the corresponding prettyprinter is used for the third task (see 
Section~\ref{impl-ccode-expr}).

Note that in both packages the main module is named \code{Language.C}. If both packages are exposed to the ghc Haskell
compiler, a package-qualified import must be used in the Haskell program, which must be enabled by a language pragma:
\begin{verbatim}
  {-# LANGUAGE PackageImports #-}
  ...
  import "language-c" Language.C
\end{verbatim}

\subsection{Including Files}
\label{impl-ccode-include}

The filter \code{gencot-include <dirlist>} processes all quoted include directives and replaces them (transitively) by the 
content of the included file. Line directives are inserted at the begin and end of an included file, so that
for all code in the output the original source file name and line number can be determined. The \code{<dirlist>}
specifies the directories to search for included files.

\subsubsection{Filter \code{gencot-include}}

The filter for expanding the include directives is implemented as an awk script, heavily inspired by the ``igawk''
example program in the gawk infofile, edition 4.2, in Section 11.3.9.

As argument it expects a directory list specified with ``:'' as separator. The list corresponds
to directories specified with the \code{-I} cpp option, it is used for searching included files.
All directories for searching included files must be specified in the arguments, there are no defaults.

Similar to cpp, a file included by a quoted directive is first searched in the directory of the including file. 
If not found there, the argument directory list is searched.

Since the input of \code{gencot-include} is read from standard input it is not associated with a directory. Hence
if files are included from the same directory, that directory must also be specified explicitly in an argument directory
list.

\subsubsection{Generating Line Directives}

Line directives are inserted into the output as follows.

If the first line of the input is a line directive, it is copied to the output. Otherwise 
the line directive
\begin{verbatim}
  # 1 "<stdin>"
\end{verbatim}
is prepended to the output.

If after a generated line directive with file name \code{\"fff\"} the input line \code{NNN} contains the 
directive 
\begin{verbatim}
  #include "filepath"
\end{verbatim}
the directive is replaced in the output by the lines 
\begin{verbatim}
  # 1 "dir/filepath" 1
  <content of file filepath>
  # NNN+1 "fff" 2
\end{verbatim}

The \code{\"dir/\"} prefix in the line directives for included files is determined as follows. 
If the included file has been found in the 
directory of its includer, the directory pathname is constructed from \code{\"fff\"} by taking the pathname 
up to and including the last ``/'' (if present, otherwise the prefix is empty).
If the included file has been found in a directory from the argument directory list
the directory pathname is used as specified in the list.

\subsubsection{Multiple Includes}

The C preprocessor does not prevent a file from being included multiple times. Usually, C include files use
an ifdef directive around all content to prevent multiple includes. The \code{gencot-include} filter does
not interprete ifdef directives, instead, it simply prevents multiple includes for all files independent 
from their contents, only based on their full file pathnames. To mimic the behavior of cpp, if a file is 
not include due to repeated include, the corresponding line directives are nevertheless generated in the form
\begin{verbatim}
  # 1 "dir/filepath" 1
  # NNN+1 "fff" 2
\end{verbatim}

\subsection{Preprocessing}
\label{impl-ccode-preproc}

The language-c parser supports an integrated invocation of an external preprocessor, the default is to use
the gcc preprocessor. However, the integrated invocation always reads the C code from a file (and checks
its file name extension) and not from standard input.

To implement C code processing as a filter, Gencot does not use the integrated preprocessor,
it invokes the preprocessor as an additional separate step. For consistency reasons it is wrapped in
the minimal filter script \code{gencot-cpp}. 

The preprocessor step only has the following purpose:
\begin{itemize}
\item process all system include directives by including the file contents,
\item process retained conditional directives to prevent conflicts in the C code.
\end{itemize}
All other preprocessing has already been done by previous steps.

\subsection{Reading the Input}
\label{impl-ccode-read}

\subsubsection{Parsing}

To apply the language-c parser to the standard input we invoke it using function \code{parseC}. It needs an \code{InputStream}
and an initial \code{Position} as arguments. 

The language-c parser defines \code{InputStream} to be the standard type \code{Data.ByteString}. To get the 
standard input as a \code{ByteString} the function \code{ByteString.getContents} can be used. 

The language-c parser uses type \code{Position} to describe a character position in a named file. It provides
the function \code{initPos} to create an initial position at the beginning of a file, taking a \code{FilePath}
as argument, which is a \code{String} containing the file name. Since Gencot and the C preprocessor create
line directives with the file name \code{<stdin>} for the standard input, this string is the correct argument
for \code{initPos}. 

The result of \code{parseC} is of type \code{(Either ParseError CTranslUnit)}. Hence it should be checked whether
an error occurred during parsing. If not, the value of type \code{CTranslUnit} is the abstract syntax tree for
the parsed C code.

Both \code{parseC} and \code{initPos} are exported by module \code{Language.C}. The function \code{ByteString.getContents}
is exported by the module \code{Data.Bytestring}. Hence to use the parser we need the following imports:
\begin{verbatim}
  import Data.ByteString (getContents)
  import "language-c" Language.C (parseC,initPos)
\end{verbatim}

Then the abstract syntax tree can be bound to variable \code{ast} using
\begin{verbatim}
  do
    input_stream <- Data.ByteString.getContents
    ast <- either (error . show) return $ parseC input_stream (initPos "<stdin>")
\end{verbatim}

\subsubsection{Analysis}

Although it is not complete and only processes toplevel declarations (including typedefs), and object definitions, the
language-c analysis module is very
useful for implementing Gencot translation. Function definition bodies are not covered by analysis, but they are
not covered by Gencot either.

The main result of the analysis module is the symbol table. Since at the end of traversing a correct C AST the toplevel
scope is reached, the symbol table only contains all globally defined identifiers. From this symbol table 
a map is created containing all toplevel declarations and object definitions, mapping the identifiers
to their semantics, which is mainly its declared type. Whereas in the abstract syntax tree there may be several declarators
in a declaration, declaring identifiers with different types derived from a common type, the map maps every identifier
to its fully derived type. 

Also, tags for structs, unions and enums are contained in the map. In C their definitions can be embedded in other declarations.
The analysis module collects all these possibly embedded declarations in the symbol table. The map also gives for
every defined type name its definition.

Together, the information in the map is much more appropriate for creating Cogent code, where all type definitions are on
toplevel. Therefore, Gencot uses the map resulting from the analysis step as starting point for its translation. 
Additionally, Gencot uses the symbol table built by the analysis module during its own processing to access the
types of globally defined identifiers and for managing local declarations when traversing function bodies, as described in
Section~\ref{impl-ccode-trav}.

Additionally, the analysis module provides a callback handler which is invoked for every declaration entered into the symbol 
table (with the exception of tag forward declarations and enumerator declarations). The callback handler can accumulate results 
in a user state which can be retrieved after analysis together with the
semantics map. Since the callback handler is also invoked for all local declarations it is useful when all declarations
shall be processed in some form.

To use the analysis module, the following import is needed:
\begin{verbatim}
  import Language.C.Analysis
\end{verbatim}

Then, if the abstract syntax tree has been bound to variable \code{ast}, it can be analysed by
\begin{verbatim}
  (table,state) <- either (error . show) return $ 
    runTrav uinit (withExtDeclHandler (analyseAST ast >> getDefTable) uhandler)
\end{verbatim}
which binds the resulting symbol table to variable \code{table} and the resulting state to \code{ustate}. \code{runTrav}
returns a result of type \code{Either [CError] (DefTable, TravState s)}, where \code{DefTable}
is the type of the symbol table and \code{s} is the type of the user state. The error list in the first alternative contains 
fatal errors which made the analysis fail. The state in the second alternative contains warnings about semantic inconsistencies, 
such as unknown identifiers, and it contains the user state. \code{uinit} is the initial user state and \code{uhandler}
is the callback handler of type
\begin{verbatim}
  DeclEvent -> Trav s ()
\end{verbatim}
It returns a monadic action without result.

The semantics map is created from the symbol table by the function \code{globalDefs}, its type is \code{GlobalDecls}.

On this basis, Gencot implements the following functions in the module \code{Gencot.Input} as utility for parsing and analysis:
\begin{verbatim}
  readFromInput :: s -> (DeclEvent -> Trav s ()) -> IO (DefTable, s)
  readFromFile :: FilePath -> s -> (DeclEvent -> Trav s ()) -> IO (DefTable, s)
\end{verbatim}
The first one takes as arguments an initial user state and a callback handler. It reads C code from standard input, parses
and analyses it and returns the symbol table and the user state accumulated by the callback handler. The second function
takes a file name as additional argument and does the same reading from the file.

All Gencot filters which read C code use one of these two functions.

\subsubsection{Source Code Origin}

The language-c parser adds information about the source code origin to the AST. For every syntactic construct represented
in the AST it includes the start origin of the first input token and the start origin and length of the last input token.
The start origin of a token is represented by the type \code{Position} and includes the original source file name and 
line number, affected by line directives if present in the input. It also includes the absolute character offset in the 
input stream. The latter can be used to determine the ordering of constructs which have been placed in the same line.
The type \code{Position} is declared as instance of class \code{ORD} by comparing the character offset, hence it can 
easily be used for comparing and sorting.

The origin information about the first and last token is contained in the type \code{NodeInfo}. All types for representing
a syntactic construct in the AST are parameterized with a type parameter. In the actual AST types this parameter is always 
substituted by the type \code{NodeInfo}. 

The analysis module carries the origin information over to its results, by including a \code{NodeInfo} in most of its
result structures. This information can be used to
\begin{itemize}
\item determine the origin file for a declared identifier,
\item filter declarations according to the source file containing them,
\item sort declarations according to the position of their first token in the source,
\item translate identifiers to file specific names to avoid conflicts.
\end{itemize}

For the last case the true name of the processed file is required, however, the parsed input is read from a pipe where
the name is always given as \code{<stdin>}. The true name is passed to the Haskell program as an additional 
argument, as described in Section~\ref{impl-comps-filters}. Since there is no easy way to replace the file name in
all \code{NodeInfo} values in the semantic map, Gencot adds the name to the monadic state used for processing
(see Section~\ref{impl-ccode-trav}).

\subsubsection{Preparing for Processing}

The main task for Gencot is to translate all declarations or definitions which are contained in a single source file, where
nested declarations are translated to a sequence of toplevel Cogent definitions. This is achieved by parsing and analysing
the content of the file and all included files, filtering the resulting set of declarations according to the source file name
\code{<stdin>}, removing all declarations which are not translated to Cogent, and sorting the remaining ones in a list. 
Translating every list entry to Cogent yields the resulting Cogent definitions in the correct ordering.

The type \code{GlobalDecls} consists of three separate maps, one for tag definitions, one for type definitions,
and one for all other declarations and definitions. Every map uses its own type for its range values, however, 
there is the wrapper type \code{DeclEvent} which has a variant for each of them. 

The language-c analysis module provides a filtering function for its resulting map of type \code{GlobalDecls}. The filter 
predicate is defined for values of type \code{DeclEvent}. If the map has been bound to the variable \code{gmap}
it can be filtered by
\begin{verbatim}
  filterGlobalDecls globalsFilter gmap
\end{verbatim}
where \code{globalsFilter} is the filter predicate.

Gencot uses a filter which reduces the declarations to those contained directly in the input file, removing all
content from included files. Since the input file is always associated with the name \code{<stdin>} in the \code{NodeInfo}
values, a corresponding filter function is
\begin{verbatim}
  (maybe False ((==) "<stdin>") . fileOfNode)
\end{verbatim}
Additionally, for a specific Gencot component, the declarations are reduced to those which are processed by the component. 

Every map range value, and hence every \code{DeclEvent} value contains the identifier which is mapped to it, 
hence the full information required for translating the definitions is contained in the range values. 
Gencot wraps every range value as a \code{DeclEvent}, and puts them in a common list for all three maps. This
is done by the function
\begin{verbatim}
  listGlobals :: GlobalDecls -> [DeclEvent]
\end{verbatim}

Finally, the declarations in the list are sorted according to the offset position of their first tokens, using the
compare function
\begin{verbatim}
  compEvent :: DeclEvent -> DeclEvent -> Ordering
  compEvent ci1 ci2 = compare (posOf ci1) (posOf ci2)
\end{verbatim}

Together, the list for processing the code is prepared from the symbol table \code{table} by
\begin{verbatim}
  sortBy compEvent $ listGlobals $ filterGlobalDecls globalsFilter $ globalDefs table
\end{verbatim}

All this preprocessing is implemented in module \code{Gencot.Input}. It provides the function
\begin{verbatim}
  getDeclEvents :: GlobalDecls -> (DeclEvent -> Bool) -> [DeclEvent]
\end{verbatim}
It performs the preprocessing and returns the list of \code{DeclEvent}s to be processed.
As its second argument it expects a predicate for filtering the content of \code{<stdin>} to the
\code{DeclEvent}s to be processed by the specific Gencot component.

\subsection{Dummy Declarations for Preprocessor Macros}
\label{impl-ccode-dummydecl}

As described in Section~\ref{design-preprocessor-macros} macro calls in C code must either be syntactically correct
C code or they must be converted to syntactically correct C code. Due to the language-c analysis step this is not 
sufficient. The analysis step checks for additional properties. In particular, it requires that every identifier 
is either declared or defined.

Thus for every identifier which is part of a converted macro call a corresponding declaration must be added to the 
C code. They are called ``dummy declarations'' since they are only used for making the analysis step happy. 

For all preprocessor defined constants Gencot automatically generates the required dummy declarations. The corresponding
macro calls always have the form of a single identifier occurring at positions where a C expression is expected. The type
of the identifier is irrelevant, hence Gencot always uses type \code{int} for the dummy declarations. For every preprocessor
constant definition of the form 
\begin{verbatim}
  #define NNN XXX
\end{verbatim}
a dummy declaration of the form
\begin{verbatim}
  int NNN;
\end{verbatim}
is generated. This is implemented by the additional filter \code{gencot-gendummydecls}. It is applied to the result of 
\code{gencot-selppconst}. The resulting dummy declarations are prepended to the input of the language-c preprocessor
since this prevents the lines from being counted for the \code{<stdin>} part.

Flag macro calls do not occur in C code, hence no dummy declarations are required for them.

For all other macros the required dummy declarations must be created manually and added to the Gencot macro call conversion.
Even if no macro call conversion is needed because the macro calls are already in C syntax, it may be necessary to
add dummy declarations to satisfy the requirements of the language-c analysis step.

\subsection{Generating Cogent Code}
\label{impl-ccode-gencog}

When Gencot generates its Cogent target code it uses the data structures defined by the Cogent compiler for representing
its AST after parsing Cogent code. The motivation to do so is twofold. First, the AST omits details such as using code layout
and parentheses for correct code structure and the Cogent compiler provides a prettyprint function for its AST which cares
about these details. Hence, it is much easier to generate the AST and use the prettyprinter for output, instead of generating
the final Cogent program text. Second, by using the Cogent AST the generated Cogent code is guaranteed to be syntactically correct and
current for the Cogent language version of the used compiler version. Whenever the Cogent language syntax is changed
in a newer version, this will be detected when Gencot is linked to the newer compiler version.

\subsubsection{Cogent Surface Syntax Tree}

The data structures for the Cogent surface syntax AST are defined in the module Cogent.Surface. It defines parameterized types
for the main Cogent syntax constructs (\code{TopLevel}, \code{Alt}, \code{Type}, \code{Polytype}, \code{Pattern}, 
\code{IrrefutablePattern}, \code{Expr}, and \code{Binding}), where the type parameters determine the types of the 
sub-structures. Hence the AST types
can easily be extended by wrapping the existing types in own extensions which are then also used as actual type parameters.

Cogent itself defines two such wrapper type families: The basic unextended types \code{RawXXX} and the types \code{LocXXX}
where every construct is extended by a representation of its source location. 

All parameterized types for syntax constructs and the \code{RawXXX} and \code{LocXXX} types are defined as instances of 
class \code{Pretty} from
module \code{Text.PrettyPrint.ANSI.Leijen}. This prettyprinter functionality is used by the Cogent compiler for outputting
the parsed Cogent source code after some processing steps, if requested by the user.

As source location representation in the \code{LocXXX} types Cogent uses the type \code{SourcePos} from Module 
\code{Text.Parsec.Pos} in package \code{parsec}.
It contains a file name and a row and column number. This information is ignored by the prettyprinter.

\subsubsection{Extending the Cogent Surface Syntax}

Gencot needs to extend the Cogent surface syntax for its generated code in two ways:
\begin{itemize}
\item origin markers must be supported, as described in Section~\ref{impl-origin},
\item C function bodies must be supported in Cogent function definitions, as described in Section~\ref{design-fundefs-body}.
\end{itemize}

\paragraph{Origin Markers}

The origin markers are used to optionally surround the generated target code parts, which may be arbitrary syntactic constructs
or groups of them. Hence it would be necessary to massively extend the Cogent surface syntax, if they are added as explicit 
syntactic constructs. Instead, Gencot optionally adds the information about the range of source lines to the syntactic
constructs in the AST and generates the actual origin markers when the AST is output. 

Although the \code{LocXXX} types already support a source position in every syntactic construct, it cannot be used by Gencot,
since it represents only a single position instead of a line range. Gencot uses the \code{NodeInfo} values, since they represent
a line range and they are already present in the C source code AST, as described in Section~\ref{impl-ccode-read}. Hence, they
can simply be transferred from the source code part to the corresponding target code part. For the case that there is no
source code part in the input file (such as for code generated for external name references), or there is no position 
information available for the source code part, the \code{NodeInfo} is optional.

It may be the case that a target AST node is generated from a source code part which is not a single source AST node. Then
there is no single \code{NodeInfo} to represent the origin markers for the target AST node. Instead, Gencot uses the 
\code{NodeInfo} values of the first and last AST nodes in the source code part.

It may also be the case that a structured source code part is translated to a sequence of sub-part translations without target
code for the main part. In this case the \code{\#ORIGIN} marker for the main part must be added before the \code{\#ORIGIN} 
marker of the first target code part and the \code{\#ENDORIG} marker for the main part must be added after the \code{\#ENDORIG} 
marker of the last target code part. 

To represent all these cases, the origin information for a construct in the target AST consists of two lists of \code{NodeInfo}
values. The first list represents the sequence of \code{\#ORIGIN} markers to be inserted before the construct, here only the
start line numbers in the \code{NodeInfo} values are used. The second list represents the sequence of \code{\#ENDORIG} markers 
to be inserted after the construct, here only the end line numbers in the \code{NodeInfo} values are used. If no marker of
one of the kinds shall be present, the corresponding list is empty.

Additional information must be added to represent the marker extensions for placing the comments (the trailing ``+'' signs).
Therefore, a boolean value is added to all list elements.

Together, Gencot defines the type \code{Origin} for representing the origin information, with the value \code{noOrigin}
for the case that no markers will be generated:
\begin{verbatim}
  data Origin = Origin { 
    sOfOrig :: [(NodeInfo,Bool)], 
    eOfOrig :: [(NodeInfo,Bool)] } 
  noOrigin = Origin [] []
\end{verbatim}
Gencot adds an \code{Origin} value to every Cogent AST element. The type \code{Origin} is defined in the module 
\code{Gencot.Origin}

\paragraph{Embedded C Code}

Cogent function definitions are represented by the \code{FunDef} alternative of the type for toplevel syntactic constructs:

\begin{verbatim}
  data TopLevel t p e = 
    ... | FunDef VarName (Polytype t) [Alt p e] | ...
\end{verbatim}
The type parameter \code{e} for representing syntactic expressions is only used in this alternative and in the alternative
for constant definitions. Cogent constant definitions are generated by Gencot only from C enum constants (preprocessor
constants are processed by \code{gencot-prcconst} which is not implemented in Haskell). The defined value for a C enum
constant is represented in the C AST by the type for expressions. Together, instead of Cogent expressions, Gencot always
uses either a C expression or a Cogent expression together with a C function body (which syntactically is a statement) 
in the Cogent AST. 

To modify the Cogent syntax in this way, Gencot defines an own expression type with two alternatives for a C expression 
and for a Cogent expression together with a C statement:
\begin{verbatim}
  data GenExpr = ConstExpr Exp
               | FunBody RawExpr Stm
\end{verbatim}
where \code{Exp} and \code{Stm} are the types for C expressions and statements as defined by the language-c-quote AST 
(see Section~\ref{impl-ccode-expr}). Note that no \code{Origin} components are added, since the types \code{Exp} and 
\code{Stm} already contain \code{Origin} information. The type \code{RawExpr} is used for the dummy result expression.
It has no origin in the C source, therefore the raw type without origin information is sufficient. 

Other than for the dummy result expression, the Cogent AST expression type is not used by Gencot. 
Since bindings only occur in expressions, the AST type for Cogent bindings is not used either.

For the type parameters \code{t} and \code{p} for representing types and patterns, respectively, the normal types for 
the Cogent constructs are used, since Gencot generates both in Cogent syntax. The pattern generated for a function
definition is always a tuple pattern, which is irrefutable. Gencot never generates other patterns, hence the AST
type for irrefutable patterns is sufficient. 

Together, Gencot uses the following types to represent its extended Cogent surface AST:
\begin{verbatim}
  data GenToplv =
    GenToplv Origin (TopLevel GenType GenIrrefPatn GenExpr)
  data GenAlt =
    GenAlt Origin (Alt GenIrrefPatn GenExpr)
  data GenIrrefPatn = 
    GenIrrefPatn Origin (IrrefutablePattern VarName GenIrrefPatn)
  data GenType = 
    GenType Origin (Type GenExpr GenType)
  data GenPolytype = 
    GenPolytype Origin (Polytype GenType)
\end{verbatim}
The first parameter of \code{Type} for expressions is only used for Cogent array types, which are currently 
not generated by Gencot.

All five wrapper types are defined as instances of class \code{Pretty}, basically by applying the Cogent prettyprint
functionality to the wrapped Cogent AST type.

\subsection{Mapping Names}
\label{impl-ccode-names}

Names used in the target code are either mapped from a C identifier or introduced, as described in 
Section~\ref{design-names}. Different schemas are used depending on the kind of name to be generated.
The schemas require different information as input.

\subsubsection{General Name Mapping}

The general mapping scheme is applied whenever a Cogent name is generated from an existing C identifier.
Its purpose is to adjust the case, if necessary and to avoid conflicts between the Cogent name and
the C identifier.

As input this scheme only needs the C identifier and the required case for the Cogent name.
It is implemented by the function
\begin{verbatim}
  mapName :: Bool -> Ident -> String
\end{verbatim}
where the first argument specifies whether the name must be uppercase.

\subsubsection{Cogent Type Names}

A Cogent type name (including the names of primitive types) may be generated as translation of a C 
primitive type, a C typedef name, a C struct/union/enum type reference, or a C derived type. 

A C primitive type is translated according to the description in Section~\ref{design-types}. Only the
type specifiers for the C type are required for that.

A C typedef name is translated by simply mapping it with the help of \code{mapName} to an uppercase name.
Only the C typedef name is required for that.

A C struct/union/enum type reference may be tagged or tagless. If it is tagged, the Cogent type name is
constructed from the tag as described in Section~\ref{design-names}: the tag is mapped with the help of
\code{mapName} to an uppercase name, then a prefix \code{Struct\_}, \code{Union\_} or \code{Enum\_} is 
prepended. For this mapping the tag and the kind (struct/union/enum) are required. Both are contained
in the language-c type \code{TypeName} which is used to represent a reference to a struct/union/enum.

If the reference is untagged, Gencot nevertheless generates a type name, as motivated and described 
in Section~\ref{design-names}. As input it needs the kind and the position of the struct/union/enum 
definition. The latter is not contained in the \code{TypeName}, it contains the position of the reference
itself. To access the position of the definition, the definition must be retrieved from the symbol table
in the monadic state. Hence, the mapping function is defined as a monadic action.

Together the function for translating struct/union/enum type references is
\begin{verbatim}
  transTagName :: TypeName -> FTrav String
\end{verbatim}

Since an untagged struct/union/enum can be contained in any type specification and type specifications
may occur in all other C constructs, the \code{GlobalDecls} map must be passed as argument to all translation
functions from C constructs to Cogent constructs.

If the definition itself is translated, it is already available and need not be retrieved from the map. 
However, as described in Section~\ref{impl-ccode-gencog}, the map may be needed to map the generic name
\code{<stdin>} to the true source file name. Therefore Gencot uses function \code{transTagName} also when
translating the definition.

A C derived type is translated to a Cogent type name by translating the name of the basic type as described
above, and then prepending the encoded sequence of derivation steps, as
described in Section~\ref{design-names}. The information about the derivation steps is contained in the 
type construct, no information in addition to that required for translating the basic type name is needed.

\subsubsection{Cogent Function Names}

Cogent function names are generated from C function names. A C function may have external or internal
linkage, according to the linkage the Cogent name is constructed either as a global name or as a name specific
to the file where the function is defined. For deciding which variant to use for a function name reference,
its linkage must be determined. It is available in the definition or in a declaration for the function name,
either of which must be present in the symbol table. The language-c analysis module replaces all 
declarations in the tyble by the
definition, if that is present in the parsed input, otherwise it retains a declaration. 

A global function name is generated by mapping the C function name with the help of \code{mapName} to
a lowercase Cogent name. No additional information is required for that.

For generating a file specific function name, the file name of the definition is required. Note that 
this is only done for a function with internal linkage, where the definitions must be present in
the input whenever the function is referenced. The definition contains the position information
which includes the file name. Hence, the symbol table is sufficient for translating the name,
to make it available the translation function is defined as a monadic action.

In C bodies function names cannot be syntactically distinguished from variable names. Therefore, Gencot
uses a common function for translating function and variable names. For a description how variable
names are translated see Section~\ref{impl-ccode-expr}.
\begin{verbatim}
  transObjName :: Ident -> FTrav String
\end{verbatim}

Similar as for tags, the function is also used when translating a function definition, although the 
definition is already available.

\subsubsection{Cogent Constant Names}

Cogent constant names are only generated from C enum constant names. They are simply translated
with the help of \code{mapName} to a lowercase Cogent name. No additional information is required.

\subsubsection{Cogent Field Names}

C member names and parameter names are translated to Cogent field names. Only if the C name is
uppercase, the name is mapped to a lowercase Cogent name with the help of \code{mapName}, 
otherwise it is used without change. Only the C name is required for that, in both cases it is
available as a value of type \code{Ident}. The translation is implemented by the function
\begin{verbatim}
  mapIfUpper :: Ident -> String
\end{verbatim}

\subsection{Generating Origin Markers}
\label{impl-ccode-origin}

For outputting origin markers in the target code, the AST prettyprint functionality must be extended.

The class \code{Pretty} used by the Cogent prettyprinter defines the methods
\begin{verbatim}
  pretty :: a -> Doc
  prettyList :: [a] -> Doc
\end{verbatim}
but the method \code{prettyList} is not used by Cogent. Hence, only the method \code{pretty} needs to be defined
for instances. The type \code{Doc} is that from module \code{Text.PrettyPrint.ANSI.Leijen}.

The basic approach is to wrap every syntactic construct in a sequence of \code{\#ORIGIN} markers and 
a sequence of \code{\#ENDORIG} markers according to the origin information for the construct in the extended AST. 
This is done by an instance definition of the form
\begin{verbatim}
  instance Pretty GenToplv where
    pretty (GenToplv org t) = addOrig org $ pretty t
\end{verbatim}
for \code{GenToplv} and analogous for the other types. The function \code{addOrig} has the type
\begin{verbatim}
  addOrig :: Origin -> Doc -> Doc
\end{verbatim}
and wraps its second argument in the origin markers according to its first argument.

The Cogent prettyprinter uses indentation for subexpressions. Indentation is implemented by the \code{Doc} type, 
where it is called ``nesting''. The prettyprinter maintains a
current nesting level and inserts that amount of spaces whenever a new line starts. 

The origin markers must be positioned in a separate line, hence \code{addOrig} outputs a newline before and after
each marker. This is done even at the beginning of a line, since due to indentation it cannot safely be determined
whether the current position is at the beginning of a line. Cogent may change the nesting of the next line after \code{addOrig}
has output a marker (typically after an \code{\#ENDORIG} marker). The newline at the end of the previous marker 
still inserts spaces according to the old nesting level, which determines the current position at the begin of
the following marker. This is not related to the new nesting level. 

This way many additional newlines are generated, in
particular an empty line is inserted between all consecutive origin markers. The additional newlines are later removed
together with the markers, when the markers are processed. Note that, if a syntactic construct
is nested, the indentation also applies to the origin markers and the line after it. To completely remove an
origin marker from the target code it must be removed together with the newline before it and with the newline 
after it and the following indentation. The following indentation can be determined since it is the same as that 
for the marker itself (a sequence of blanks of the same length). 

\subsubsection{Repeated Origin Markers}

Normally, target code is positioned in the same order as the corresponding source code. This implies, that
origin markers are monotonic. A repeated origin marker is a marker with the same line number as its previous marker.
Repeated origin markers of the same kind must be avoided, since they would result in duplicated comments or 
misplaced directives.
Repeated origin markers of the same kind occur, if a subpart of a structured source code part begins or ends 
in the same line as its main part. In this case only the outermost markers must be retained.

An \code{\#ENDORIG} marker repeating an \code{\#ORIGIN} marker means that the source code
part occupies only one single line (or a part of it), this is a valid case. 
An \code{\#ORIGIN} marker repeating an \code{\#ENDORIG} marker means that the previous source code
part ends in the same line where the following source code part begins. In this case the markers are
irrelevant, since no comments or directives can be associated with them. However, if they are
present they introduce unwanted line breaks, hence they also are avoided by removing both of them.

Together, the following rules result. In a sequence of repeated \code{\#ORIGIN} markers, only the first one 
is generated. In a sequence of repeated \code{\#ENDORIG} markers only the last one is generated.
If an \code{\#ORIGIN} marker repeats an \code{\#ENDORIG} marker, both are omitted.

There are several possible approaches for omitting repeated origin markers:
\begin{itemize}
\item omit repeated markers when building the Cogent AST
\item traverse the Cogent AST and remove markers to be omitted
\item output repeated markers and remove them in a postprocessing step
\end{itemize}
Note, that it is not possible to remove repeated markers already in the language-c AST, since there a \code{NodeInfo}
value always corresponds to two combined markers.

Handling repeated markers in the Cogent AST is difficult, because for an \code{\#ORIGIN} marker the context
before it is relevant whereas for an \code{\#ENDORIG} marker the context after it is relevant. An additional
AST traversal would be required to determine the context information. The first approach is even more complex
since the context information must be determined from the source code AST where the origin markers are not
yet present. 

For this reason Gencot uses the third approach and processes repeated markers in the generated target code text,
independent from the syntactical structure.

\subsubsection{Filter for Repeated Origin Marker Elimination}

The filter \code{gencot-reporigs} is used for removing repeated origin markers. It is implemented as an awk script.

It uses five string buffers: two for the previous two origin markers read, and three for the code before,
between, and after both markers. Whenever all buffers are filled (the buffer after both markers with a 
single text line; this line exists, since consecutive markers are always separated by an empty line),
the markers are processed as follows, if they have the same line number: in the case of two 
\code{\#ORIGIN} markers the second is deleted, in the
case of two \code{\#ENDORIG} markers the first is deleted, and in the case of an \code{\#ORIGIN}
marker after an \code{\#ENDORIG} marker both are deleted. In the latter case the line number of the
\code{\#ORIGIN} marker is remembered and subsequent \code{\#ORIGIN} markers with the same line number
are also deleted.

When both markers have different line numbers or if an \code{\#ENDORIG} marker follows an \code{\#ORIGIN}
marker the first marker and the code before it are output and the buffers are filled until the next marker
has been read.

\subsection{Generating Expressions}
\label{impl-ccode-expr}

For outputting the Cogent AST the prettyprint functionality must be extended to 
output C function bodies and the C expressions used for constant definitions. Additionally, at least in function bodies,
origin markers must be generated to be able to re-insert comments and preprocessor directives. Finally, all names
occurring free in a function body or a constant expression must be mapped to Cogent names.

The language-c prettyprinter is defined in module \code{Language.C.Pretty}. It defines an own class \code{Pretty} with 
method \code{pretty} to convert the AST types to a \code{Doc}. However, other than the Cogent prettyprinter, it uses 
the type \code{Doc} from module \code{Text.PrettyPrint.HughesPJ} instead of module \code{Text.PrettyPrint.ANSI.Leijen}.
This could be adapted by rendering the \code{Doc} as a string and then prettyprinting this string to a \code{Doc}
from the latter module. This way, a prettyprinted function body could be inserted in the document created by the
Cogent prettyprinter.

\subsubsection{Origin Markers}

For generating origin markers, a similar approach is not possible, since they must be inserted between single statements,
hence, the function \code{pretty} must be extended. Although it does not use the \code{NodeInfo}, it is only defined for
the AST type instances with a \code{NodeInfo} parameter and has no genericity which could be exploited for extending it.
Therefore, Gencot has to fully reimplement it. 

In the prettyprint reimplementation the target code parts must be wrapped by origin markers
in the same way as for the Cogent AST. However, for the type \code{Doc} from module \code{Text.PrettyPrint.HughesPJ} 
this is not possible, since newlines are only
available as separators between documents and cannot be inserted before or after a document. An alternative choice
would be to use the type \code{Doc} from \code{Text.PrettyPrint.ANSI.Leijen}, as the Cogent prettyprinter does.
However, the approach of both modules is quite different so that it would be necessary to write a new C 
prettyprint implementation nearly from scratch. 

It has been decided to use another approach which is expected to be simpler. The alternative C parser language-c-quote 
also has a prettyprinter. It generates a type \code{Doc} defined by a third module \code{Text.PrettyPrint.Mainland}.
It is similar to \code{Text.PrettyPrint.ANSI.Leijen} and also supports adding newlines before and after a document.
The language-c-quote prettyprinter is defined in the module \code{Language.C.Pretty} of language-c-quote and consists
of the method \code{ppr} of the class \code{Pretty} defined in module \code{Text.PrettyPrint.Mainland.Class.Pretty}.
This method is not generic at all, hence it must be completely reimplemented to extend it for generating origin 
markers. However, this reimplementation is straightforward and can be done by copying the original implementation
and only adding the origin marker wrappings. The resulting Gencot module is \code{Gencot.C.Output}.

Whereas the type \code{Doc} from \code{Text.PrettyPrint.ANSI.Leijen} provides a \code{hardline} document which always
causes a newline in the output, the type \code{Doc} from \code{Text.PrettyPrint.Mainland} does not. Normal line breaks
are ignored in certain contexts, if there is enough room. Using normal line breaks around origin markers could result
in origin markers with other code in the same line before or after the marker.

For the reimplemented language-c-quote prettyprinter Gencot defines its own \code{hardline} by using a newline 
which is hidden for type \code{Doc}. This could be implemented without nesting the marker and the subsequent line.
However, if at the marker position a comment is inserted, the subsequent line should be correctly indented.
To achieve this, the \code{hardline} implementation also adds the current nesting after the newline.

Hiding the newline from \code{Doc} implies that the ``current column'' maintained by \code{Doc} is not
correct anymore, since it is not reset by the \code{hardline}. Every \code{hardline} will instead advance the current
column by the width of the marker and twice the current nesting. This has two consequences.

First, in some places the language-c-quote prettyprinter uses ``alignment'' which means an indentation of subsequent lines
to the current column. This indentation will be too large after inserted markers. Gencot handles this by replacing 
alignment everywhere in the prettyprint implementation by a nesting of two additional columns. 

Second, the language-c-quote prettyprinter is parameterized by a ``document width''. It automatically breaks lines 
when the current column exceeds the document width. The incorrect column calculation causes many additional such
line breaks, since the current column increases much faster than normal. Gencot handles this by setting the document
width to a very large value (such as 2000 instead of 80) to compensate for the fast column increase.

\subsubsection{Using the language-c-quote AST}

Language-c-quote uses a different C AST implementation than language-c. To use its reimplemented prettyprinter, the 
language-c AST must be translated to a language-c-quote AST. This is not trivial, since the structures are somewhat
different, but it seems to be simpler than implementing a new C prettyprinter. The translation is implemented in
the module \code{Gencot.C.Translate}. 

Additionally the language-c-quote AST must be extended by \code{Origin} values. The language-c-quote AST already 
contains \code{SrcLoc} values which are similar to the \code{NodeInfo} values in language-c. Like these they cannot
be used as origin marker information since they cannot represent begin and end markers independently. Therefore
Gencot also reimplements the language-c-quote AST by copying its data types and replacing the \code{SrcLoc}
values by \code{Origin} values. This is implemented in module \code{Gencot.C.Ast}.

Together, this approach yields a similar structure as for the translation to Cogent: The Cogent AST is extended 
by the structures in \code{Gencot.C.Ast} to represent function bodies and constant expressions. The function for
translating from language-c AST to the Cogent AST is extended by the functions in \code{Gencot.C.Translate} to
translate function bodies and constant expressions from the language-c AST to the reimplemented language-c-quote 
AST, and the Cogent prettyprinter is extended by the prettyprinter
in \code{Gencot.C.Output} to print function bodies and constant expressions with origin markers.

In addition to translating the C AST structures from language-c to those of language-c-quote, the translation
function in \code{Gencot.C.Translate} implements the following functionality:
\begin{itemize}
\item generate \code{Origin} values from \code{NodeInfo} values,
\item map C names to Cogent names.
\end{itemize}

\subsubsection{Name Mapping}

Name mapping depends on the kind of name and may additionally depend on its type. Both information is
available in the symbol table (see Section~\ref{impl-ccode-trav}). However, the scope cannot be queried
from the symbol table. Hence it is not possible to map names depending on whether they are locally defined
or globally.

The following kinds of names may occur in a function body: primitive types, typedefs, tags, members, 
functions, global variables, enum constants, preprocessor constants, parameters and local variables.

Primitive type names and typedef names can only occur as name of a base type in a declaration. Primitive
type names are mapped to Cogent primitive type names as described in Section~\ref{design-types-prim}.

A typedef name may also occur in a declarator of a local typedef which defines the name. 
In both cases, as described in Section~\ref{design-fundefs-body}, Gencot
only maps the plain typedef names, not the derived types. The typedef names are mapped according to
Section~\ref{design-types-typedef}: If they ultimately resolve to a struct, union, or array type they
are mapped with an unbox operator applied, otherwise they are mapped without.

A tag name can only occur as base type in a declaration. It is always mapped to a name with a prefix 
of \code{Struct\_}, \code{Union\_}, or \code{Enum\_}. Tagless structs/unions/enums are not mapped at all.
Tag names are mapped according to Sections~\ref{design-types-enum} and~\ref{design-types-struct}: struct
and union tags are mapped with an unbox operator applied, enum tags are mapped without.

Gencot also maps defining tag occurrences. Thus an occurrence of the form 
\begin{verbatim}
  struct s { ... }
\end{verbatim}
is translated to
\begin{verbatim}
  struct #Struct_s { ... }
\end{verbatim}

Every occurrence of a field name can be syntactically distinguished. It is mapped according to 
Section~\ref{design-names} to a lowercase Cogent name if it is uppercase, otherwise it is unchanged.
Field names are also mapped in member declarations in locally defined structures and unions.

All other names syntactically occur as a primary expression. They are mapped depending on their semantic
information retrieved from the symbol table. In a first step it distinguishes objects, functions, 
and enumerators.

An object identifier may be a global variable, parameter, or local variable. It may also be a preprocessor 
constant since for them dummy declarations have been introduced which makes them appear as a global variable
for the C analysis. For the mapping the linkage is relevant, this is also available from the symbol table.

Identifiers for global variables may have external or internal linkage and are mapped depending on the
linkage. Identifiers for parameters always have no linkage and are always mapped like field names. Identifiers
for local variables either have no linkage or external linkage. In the first case they are mapped like
field names. In the second case they cannot be distinguished from global variables with external linkage,
and are mapped to lowercase. The dummy declarations introduced for preprocessor constants
always have external linkage, the identifiers are mapped to lowercase. Together, object identifiers with 
internal linkage are mapped as described in Section~\ref{design-names}, object identifiers with external
linkage are mapped to lowercase, and object identifiers with no linkage are mapped to lowercase if they are
uppercase and remain unchanged otherwise.

An identifier for a function has either internal or external linkage and is mapped depending on its linkage.
An identifier for an enumerator is always mapped to lowercase, like preprocessor constants.

Identifiers for local variables may also occur in a declarator of a local object definition which defines 
the name. They are also mapped depending on their linkage, as described above.

\subsection{Traversing the C AST}
\label{impl-ccode-trav}

The package language-c uses a monad for traversing and analysing the C AST. The monad is defined in module 
\code{Language.C.Analysis.TravMonad} and mainly provides the symbol table and user state during the traversal.
The traversal itself is implemented by a recursive descent according to the C AST using a separate function
for analysing every syntactic construct. 

When processing the semantic map resulting from the language-c analysis Gencot implements a similar recursive 
descent using a processing function for every syntactic construct. For this it uses the same monad for two reasons.
\begin{itemize}
\item the definitions and declarations for the global identifiers are needed for mapping the identifiers to
Cogent names,
\item additionally, the definitions and declarations for locally defined identifiers are needed in C function
bodies for mapping the identifiers.
\end{itemize}

The global definitions and declarations in the symbol table correspond to the semantics map which is the result 
of the language-c analysis step. It is created from the symbol table after traversing the C AST. Although Gencot 
processes the content of the semantics map, it is not available as a whole in the processing functions. Instead
of passing the semantics map as an explicit parameter to all processing functions, Gencot uses a monadic traversal
through the relevant parts of the semantics map, which implicitly makes the symbol table available to all 
processing functions. This is achieved by reusing the symbol table after the analysis phase for the traversal
of the semantics map.

Additionally, when processing the C function bodies, the symbol table is used for managing the local declarations. 
This is possible because although the analysis phase translates global declarations and definitions to a 
semantic representation, it does not modify function bodies and returns them as the original C AST. Since
the information about local declarations is discarded at the end of its scope, the information is not 
present anymore in the symbol table after the analysis phase. Gencot uses the symbol table functionality
to rebuild this information during its own traversal and uses it for name mapping.

The user state is used by Gencot to provide additional information, depending on the purpose of the traversal.
A common case is to make the actual name of the processed file available during processing. In the \code{NodeInfo}
values in the AST it is always specified as \code{<stdin>} since the input is read from a pipe. All C processing 
filters take the name of the original C source file as an additional argument. It is added to the user state 
of traversal monads so that it can be used during traversal.

This is supported by defining in module \code{Gencot.Traversal} the class \code{FileNameTrav} as
\begin{verbatim}
  class (Monad m) => FileNameTrav m where
    getFileName :: m String
\end{verbatim}
so that the method \code{getFileName} can be used to retrieve the source file name from all traversal monads of 
this class.

In module \code{Gencot.Names} the monadic action \code{srcFileName} is defined to returns the file name for a 
\code{NodeInfo} value and replace it by the original source file name if it is equal to \code{<stdin>}.

The utilities for the monadic traversal of the semantics map are defined in module \code{Gencot.Traversal}. 
The minimal monadic type is defined as
\begin{verbatim}
  type FTrav = Trav String
\end{verbatim}
where \code{String} is the type used for storing the original C source file name in the user state. It is an
instance of \code{FileNameTrav}. As execution 
function for the monadic actions the functions
\begin{verbatim}
  runFTrav :: FTrav a -> IO a
  runWithTable :: DefTable -> String -> FTrav a -> IO a
\end{verbatim}
are defined. The second one takes the symbol table and the original C source file name as arguments to initialize 
the state. The first one leaves both empty. The functions are themselves 
\code{IO} actions and print error messages generated during traversal to the standard output.

In the monadic actions the symbol table can be accessed by actions defined in the modules 
\code{Language.C.Analysis.TravMonad} and \code{Language.C.Analysis.DefTable}. An identifier can be
resolved using the actions
\begin{verbatim}
  lookupTypeDef :: Ident -> FTrav Type
  lookupObject :: Ident -> FTrav (Maybe IdentDecl)
\end{verbatim}
For resolving tag definitions the symbol table must be retrieved by
\begin{verbatim}
  getDefTable :: FTrav DefTable
\end{verbatim}
then the struct/union/enum reference can be resolved by
\begin{verbatim}
  lookupTag :: SUERef -> DefTable -> Maybe TagEntry
\end{verbatim}

Additionally, there are actions to enter and leave a scope and actions for inserting definitions.

An error can be recorded in the monad using the action
\begin{verbatim}
  recordError :: Language.C.Data.Error.Error e => e -> m () 
\end{verbatim}

\subsection{Creating the C Call Graph}
\label{impl-ccode-callgraph}

In some Gencot components we use the C call graph. This is the mapping from functions to the functions
invoked in their body. Here we describe the module \code{Gencot.Util.CallGraph} which provides
utility functions for creating the call graph.

The set of invoked functions is determined by traversing the bodies of all function definitions after the analysis
phase. The callback handler is not used since it is only invoked for declarations and definitions and does not help
for processing function invocations.

Invocations can be identified purely syntactically as C function call expressions. The invoked function is usually 
specified by an identifier, however, it can be specified as an arbitrary C expression. The expression can use 
identifiers which are locally declared, such as a parameter of the function where the invocation occurs. The 
declaration information of these identifiers would not be available after the traversal which builds the call graph.

To make the full information about the invoked functions available, Gencot inserts the declarations into the call graph 
instead of the identifiers. This cannot be done for arbitrary expressions, hence Gencot only handles two special cases 
of specifying the invoked function: as a single (globally or locally declared) identifier or as a member access from
a single identifier. All other invocation where the invoked function is specified in a different way are ignored and
not added to the call graph.

The call graph has the form of a mapping from function names (invoking function) to sets of declarations
(invoked functions):
\begin{verbatim}
  type CallGraph = Map Ident (Set CGDecl)
  data CGDecl =
    IdentDecl IdentDecl
    MemberDecl MmberDecl
\end{verbatim}
An invoked function can be specified by an \code{IdentDecl} (function or object of function pointer type),
or by a \code{MemberDecl} (struct or union member of function pointer type). Note that in a function definition
the parameters are represented in the symbol table by \code{IdentDecl}s, not by \code{ParamDecl}s.

To also access the declarations of locally declared identifiers, the symbol table with local declarations
must be available while building the call graph. Therefore we traverse the function bodies with the help of
the \code{FTrav} monad and \code{runWithTable} as described in Section~\ref{impl-ccode-trav}.

The call graph is constructed by the monadic action
\begin{verbatim}
  getCallGraph :: [DeclEvent] -> FTrav CallGraph
\end{verbatim}
It processes all function definitions in its argument list and ignores all other \code{DeclEvent}s.

As a utility the monadic action
\begin{verbatim}
  getInvocations :: [DeclEvent] -> FTrav (Set CGDecl)
\end{verbatim}
returns the union of all declarations of invoked functions in the call graph. It is implemented by an own
traversal of the function bodies, directly building the union of all invocations.

The declaration of an invoked function also tells 
whether the function or object is defined or only declared. Note that the traversal for collecting invocations is a ``second 
pass'' through the C source after the analysis phase of language-c. During analysis language-c replaces
declarations in the symbol table whenever it finds the corresponding definition.
